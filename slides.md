---
theme: ./
disableAnimations: true
layout: cover
title: 'Kueue & Volcano è°ƒåº¦å®æˆ˜'
subtitle: 'æ·±å…¥å¯¹æ¯”ä¸åº”ç”¨'
date: '2025-06-18'
author: "èˆ¹é•¿"
showLogo: false
showPoweredBy: false
---

---
layout: intro
avatar: https://avatars.githubusercontent.com/u/13782141?v=4
title: èˆ¹é•¿ @samzong
subtitle: èµ„æ·±äº‘åŸç”Ÿ åŠ AI å¹³å°äº§å“æ¶æ„ä¸å¸ƒé“å¸ˆ
tags: 
  - "Cloud-Native"
  - "Kubernetes" 
  - "LLM Infra"
github: samzong
techStack:
  - name: Kubernetes
    icon: https://raw.githubusercontent.com/kubernetes/kubernetes/master/logo/logo.svg
  - name: Docker
    icon: https://www.docker.com/wp-content/uploads/2022/03/Moby-logo.png
  - name: Go
    icon: https://go.dev/images/go-logo-blue.svg
  - name: Python
    icon: https://www.python.org/static/community_logos/python-logo-master-v3-TM.png
certifications:
  - name: CKA
    badge: https://images.credly.com/size/680x680/images/8b8ed108-e77d-4396-ac59-2504583b9d54/cka_from_cncfsite__281_29.png
  - name: CKAD
    badge: https://images.credly.com/size/680x680/images/cc8adc83-1dc6-4d57-8e20-22171247e052/blob
---

ç²¾é€šäº‘åŸç”ŸæŠ€æœ¯æ ˆï¼Œä¸“æ³¨äº Kubernetes ç”Ÿæ€ç³»ç»Ÿå’Œ AI åŸºç¡€è®¾æ–½å»ºè®¾ï¼›æ‹¥æœ‰ä¸°å¯Œçš„å¤§è§„æ¨¡åˆ†å¸ƒå¼ç³»ç»Ÿæ¶æ„ç»éªŒï¼Œåœ¨å®¹å™¨åŒ–ã€å¾®æœåŠ¡ã€DevOps å’Œ MLOps é¢†åŸŸæœ‰æ·±å…¥ç ”ç©¶ã€‚

---
layout: toc
---

1.  **èƒŒæ™¯**: Kubernetes æ‰¹å¤„ç†è°ƒåº¦çš„æŒ‘æˆ˜ä¸æ¼”è¿›
2.  **Kueue æ·±åº¦è§£æ**: è®¾è®¡ç†å¿µã€æ ¸å¿ƒæ¶æ„ä¸å®æˆ˜
3.  **Volcano æ·±åº¦è§£æ**: è®¾è®¡ç†å¿µã€æ ¸å¿ƒæ¶æ„ä¸å®æˆ˜
4.  **å¯¹æ¯”åˆ†æ**: Kueue vs. Volcano
5.  **AI èµ„æºä¼˜åŒ–ç­–ç•¥**: æ··éƒ¨è°ƒåº¦ã€å¼¹æ€§ä¼¸ç¼©ã€èµ„æºè¶…å–
6.  **ååŒè°ƒåº¦**: è®­ç»ƒä¸æ¨ç†å…±å­˜çš„æœ€ä½³å®è·µ
7.  **å®æˆ˜æ¼”ç¤º**: ä»»åŠ¡å…¥é˜Ÿã€è°ƒåº¦ã€æ‰§è¡Œå…¨è¿‡ç¨‹
8.  **æ€»ç»“ä¸å±•æœ›**

---
layout: chapter
part: 1
title: Kubernetes åŸç”Ÿè°ƒåº¦
---

<!--
å»ºè®®ï¼š
- èƒŒæ™¯ä½¿ç”¨åŠé€æ˜çš„ K8s scheduler æ¶æ„å›¾
- æ·»åŠ å…³é”®ç—›ç‚¹çš„å¯è§†åŒ–è¡¨ç¤º
-->

---
layout: image-right
title: Kubernetes åŸç”Ÿè°ƒåº¦çš„å±€é™æ€§
image: public/scheduling-framework-extensions.png
---

`default-scheduler` ä¸ºé€šç”¨åœºæ™¯è®¾è®¡ï¼Œä½†åœ¨æ‰¹å¤„ç†é¢†åŸŸåŠ›ä¸ä»å¿ƒ

<br />

1. **Pod-by-Pod è°ƒåº¦**: æ— æ³•ä¿è¯ä½œä¸šçš„ "All-or-Nothing"ï¼Œå¯¼è‡´èµ„æºæ­»é”å’Œæµªè´¹ã€‚
2. **ç¼ºä¹å…¬å¹³æ€§**: æ— æ³•åœ¨å¤šç§Ÿæˆ·é—´å®ç°å…¬å¹³çš„èµ„æºå…±äº«ã€‚
3. **æ— é˜Ÿåˆ—æ¦‚å¿µ**: ä½œä¸šæäº¤åç«‹å³ç«äº‰èµ„æºï¼Œé«˜è´Ÿè½½ä¸‹å¯¼è‡´é›†ç¾¤æ··ä¹±ã€‚
4. **èµ„æºç¢ç‰‡åŒ–**: é¢‘ç¹åˆ›å»ºå’Œé”€æ¯ Pod å¯¼è‡´èµ„æºåˆ©ç”¨ç‡ä½ä¸‹ã€‚

<!--
å»ºè®®ï¼š
- ä½¿ç”¨é—®é¢˜-å½±å“çš„å¯¹æ¯”å¸ƒå±€
- æ¯ä¸ªå±€é™æ€§é…å›¾æ ‡è¯´æ˜
- æ·»åŠ å®é™…æ¡ˆä¾‹æ•°æ®æ”¯æ’‘
-->

---
layout: table
title: æ‰¹å¤„ç†è°ƒåº¦çš„æ ¸å¿ƒæŒ‘æˆ˜ä¸ä¸šç•Œè§£å†³æ–¹æ¡ˆ
---

| æ ¸å¿ƒæŒ‘æˆ˜ | é—®é¢˜æè¿° | Kueue æ–¹æ¡ˆ | Volcano æ–¹æ¡ˆ | å…¶ä»–æ–¹æ¡ˆ |
|:---------|:---------|:-----------|:-------------|:----------|
| **èµ„æºç«äº‰** | å¦‚ä½•åœ¨å¤šç§Ÿæˆ·é—´å…¬å¹³åˆ†é…èµ„æºï¼Ÿ | ClusterQueue + LocalQueue<br/>åˆ†å±‚é˜Ÿåˆ—ç®¡ç† | Queue æƒé‡æœºåˆ¶<br/>å¼¹æ€§èµ„æºå€Ÿç”¨ | **Apache YuniKorn**:<br/>å±‚æ¬¡åŒ–é˜Ÿåˆ—ï¼Œæœ€å¤§/æœ€å°èµ„æºä¿è¯ |
| **ä¼˜å…ˆçº§ç®¡ç†** | å¦‚ä½•ç¡®ä¿å…³é”®ä½œä¸šä¼˜å…ˆæ‰§è¡Œï¼Ÿ | WorkloadPriorityClass<br/>èµ„æºæŠ¢å æœºåˆ¶ | PriorityClass<br/>ä½œä¸šä¼˜å…ˆçº§æ’åº | **Slurm**:<br/>å¤šçº§ä¼˜å…ˆçº§é˜Ÿåˆ— + å…¬å¹³å…±äº«ç®—æ³• |
| **èµ„æºåˆ©ç”¨ç‡** | å¦‚ä½•å‡å°‘ç¢ç‰‡åŒ–ï¼Œæå‡é›†ç¾¤æ•ˆç‡ï¼Ÿ | èµ„æºå€Ÿç”¨ (lendingLimit)<br/>åŠ¨æ€å›æ”¶æœºåˆ¶ | Gang Scheduling<br/>èµ„æºæ‰¹é‡åˆ†é… | **K8s Scheduler**:<br/>Bin Packing + èŠ‚ç‚¹äº²å’Œæ€§ |
| **å¤æ‚ä¾èµ–** | å¦‚ä½•å¤„ç†ä½œä¸šé—´çš„ä¾èµ–å…³ç³»ï¼Ÿ | åŸºç¡€æ”¯æŒ<br/>éœ€å¤–éƒ¨ç¼–æ’å·¥å…· | åŸç”Ÿæ”¯æŒä½œä¸šä¾èµ–å›¾<br/>ä»»åŠ¡æ‹“æ‰‘ç®¡ç† | **Argo Workflows**: DAGç¼–æ’<br/>**Apache Airflow**: æ¡ä»¶è§¦å‘ |
| **å¼‚æ„èµ„æº** | å¦‚ä½•æ”¯æŒ GPUã€TPU ç­‰ç‰¹æ®Šç¡¬ä»¶ï¼Ÿ | ResourceFlavor<br/>æŠ½è±¡ç¡¬ä»¶ç±»å‹ | Device Plugin é›†æˆ<br/>æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦ | **NVIDIA GPU Operator**:<br/>GPU è‡ªåŠ¨å‘ç°å’Œç®¡ç† |

---
layout: timeline
title: æ‰¹å¤„ç†è°ƒåº¦å™¨çš„æ¼”è¿›
---

## 2015-2017

Kubernetes åˆåˆ›æ—¶æœŸ

## 2018-2020

æ‰¹å¤„ç†éœ€æ±‚æ¶Œç°ï¼Œç¤¾åŒºå¼€å§‹æ¢ç´¢æ‰¹å¤„ç†è°ƒåº¦è§£å†³æ–¹æ¡ˆï¼ŒAI/ML å·¥ä½œè´Ÿè½½å¢åŠ 

## 2021 - 2024

Kueue å’Œ Volcano é¡¹ç›®ç›¸ç»§å¼€æº

## 2024

Kueue v1.0 GA

## ç°åœ¨

Kueue å’Œ Volcano æˆä¸ºä¸»æµæ‰¹å¤„ç†è°ƒåº¦æ–¹æ¡ˆ

<!--
å»ºè®®ï¼š
- æ·»åŠ Kueue v1.0 GA (é¢„è®¡2024)
- æ·»åŠ Volcano v1.9+ ç‰¹æ€§
- æ ‡æ³¨CNCFé¡¹ç›®æˆç†Ÿåº¦å˜åŒ–
-->


---
layout: boxes
title: AI/ML å·¥ä½œè´Ÿè½½çš„éœ€æ±‚
---

## å¤§è§„æ¨¡å¹¶è¡Œ

è®­ç»ƒä½œä¸šéœ€è¦æ•°ç™¾ GPU åŒæ­¥å·¥ä½œï¼Œè¦æ±‚è°ƒåº¦å™¨èƒ½å¤ŸåŒæ—¶åˆ†é…å¤§é‡èµ„æº

**æŠ€æœ¯æŒ‘æˆ˜**ï¼š
- **All-Reduce é€šä¿¡ç“¶é¢ˆ**ï¼š8å¡è®­ç»ƒéœ€ 200Gbps InfiniBandï¼Œå»¶è¿Ÿ < 10Î¼s
- **æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦**ï¼šåŒæœºæ¶ GPU é€šä¿¡å¸¦å®½æ¯”è·¨æœºæ¶é«˜ 10x
- **èµ„æºç¢ç‰‡åŒ–**ï¼š95% çš„è°ƒåº¦å¤±è´¥æºäºæ— æ³•æ‰¾åˆ°è¿ç»­çš„ GPU èµ„æºå—

## é•¿å‘¨æœŸä½œä¸š

è®­ç»ƒå¯èƒ½æŒç»­æ•°å‘¨ï¼Œéœ€è¦ç¨³å®šè°ƒåº¦å’Œèµ„æºä¿éšœï¼Œé¿å…ä¸­æ–­

**ç”Ÿäº§æ•°æ®**ï¼š
- **GPT-3 è®­ç»ƒ**ï¼š175B å‚æ•°ï¼Œéœ€è¦ 1024 ä¸ª A100 GPU è¿è¡Œ 34 å¤©
- **æ•…éšœç‡**ï¼šå¤§è§„æ¨¡é›†ç¾¤æ¯å¤©çº¦ 2-3% èŠ‚ç‚¹æ•…éšœç‡
- **æ£€æŸ¥ç‚¹å¼€é”€**ï¼šæ¯ 4 å°æ—¶ä¿å­˜æ£€æŸ¥ç‚¹ï¼Œå•æ¬¡è€—æ—¶ 15-30 åˆ†é’Ÿ

## èµ„æºå¼¹æ€§

éœ€åŠ¨æ€è°ƒæ•´èµ„æºä»¥åº”å¯¹å³°å€¼éœ€æ±‚ï¼Œæ”¯æŒå¼¹æ€§æ‰©ç¼©å®¹

**å¼¹æ€§ç­–ç•¥**ï¼š
- **é¢„çƒ­æ± è®¾è®¡**ï¼šä¿æŒ 10-20% æ¸©å¤‡å®¹é‡ï¼Œå¯åŠ¨æ—¶é—´ä» 5min é™è‡³ 30s
- **ä¼˜å…ˆçº§é˜Ÿåˆ—**ï¼šæ¨ç†ä»»åŠ¡ P0ï¼Œåœ¨çº¿è®­ç»ƒ P1ï¼Œç¦»çº¿è®­ç»ƒ P2
- **æ½®æ±è°ƒåº¦**ï¼šç™½å¤©æ¨ç†èµ„æº 70%ï¼Œå¤œé—´è®­ç»ƒèµ„æº 80%

## æ•…éšœæ¢å¤

éœ€æ”¯æŒæ£€æŸ¥ç‚¹å’Œä½œä¸šæ¢å¤ï¼Œç¡®ä¿è®­ç»ƒè¿›åº¦ä¸ä¸¢å¤±

**å®¹é”™æœºåˆ¶**ï¼š
- **ä¸‰çº§æ£€æŸ¥ç‚¹**ï¼šå†…å­˜çº§(ç§’çº§)ã€SSDçº§(åˆ†é’Ÿçº§)ã€å¯¹è±¡å­˜å‚¨(å°æ—¶çº§)
- **å¼¹æ€§è®­ç»ƒ**ï¼šåŸºäº Horovod Elasticï¼Œæ”¯æŒåŠ¨æ€å¢å‡ Worker
- **å¿«é€Ÿæ¢å¤**ï¼šä½¿ç”¨ RDMA åŠ é€Ÿæ£€æŸ¥ç‚¹æ¢å¤ï¼Œé™ä½æ¢å¤æ—¶é—´ 70%

---
layout: default
title: AI/ML å·¥ä½œè´Ÿè½½çš„æŠ€æœ¯è¦æ±‚è¯¦è§£
---

## 1. ç½‘ç»œæ‹“æ‰‘ä¸å¸¦å®½éœ€æ±‚

```mermaid
graph TD
    subgraph "è®­ç»ƒé›†ç¾¤æ‹“æ‰‘"
        A[Spine Switch<br/>400Gbps] --> B[Leaf Switch 1<br/>100Gbps]
        A --> C[Leaf Switch 2<br/>100Gbps]
        B --> D[GPU Node 1<br/>8x A100]
        B --> E[GPU Node 2<br/>8x A100]
        C --> F[GPU Node 3<br/>8x A100]
        C --> G[GPU Node 4<br/>8x A100]
    end
```

**å…³é”®æŒ‡æ ‡**ï¼š
- **èŠ‚ç‚¹å†…é€šä¿¡**ï¼šNVLink 3.0 æä¾› 600GB/s å¸¦å®½
- **è·¨èŠ‚ç‚¹é€šä¿¡**ï¼šRoCE v2 æä¾› 200Gbpsï¼Œå»¶è¿Ÿ < 2Î¼s
- **æ¢¯åº¦åŒæ­¥å¼€é”€**ï¼šå æ€»è®­ç»ƒæ—¶é—´çš„ 15-25%

## 2. GPU èµ„æºè°ƒåº¦ç®—æ³•

```go
// Kueue ä¸­çš„ GPU äº²å’Œæ€§è°ƒåº¦ç®—æ³•
func (s *Scheduler) findBestGPUPlacement(workload *Workload) (*Placement, error) {
    // 1. æ”¶é›†å¯ç”¨ GPU æ‹“æ‰‘ä¿¡æ¯
    topology := s.clusterTopology.GetGPUTopology()
    
    // 2. è®¡ç®—æœ€ä¼˜æ”¾ç½®ç­–ç•¥
    for _, node := range topology.Nodes {
        if node.AvailableGPUs >= workload.RequiredGPUs {
            // æ£€æŸ¥ NVLink è¿æ¥æ€§
            if s.checkNVLinkConnectivity(node, workload.RequiredGPUs) {
                return &Placement{
                    Node: node,
                    GPUs: s.selectOptimalGPUs(node, workload),
                }, nil
            }
        }
    }
    
    // 3. é™çº§åˆ°è·¨èŠ‚ç‚¹è°ƒåº¦
    return s.findCrossNodePlacement(workload)
}
```

## 3. æ€§èƒ½åŸºå‡†æµ‹è¯•æ•°æ®

| æ¨¡å‹è§„æ¨¡ | GPU é…ç½® | æ‰¹æ¬¡å¤§å° | ååé‡(samples/s) | GPU åˆ©ç”¨ç‡ | é€šä¿¡å¼€é”€ |
|---------|---------|---------|-----------------|-----------|---------|
| BERT-Large | 8x V100 | 64 | 1,245 | 92% | 18% |
| GPT-3 13B | 16x A100 | 32 | 487 | 87% | 28% |
| T5-11B | 32x A100 | 16 | 892 | 85% | 32% |
| LLaMA 70B | 64x H100 | 8 | 234 | 83% | 35% |

---
layout: chapter
part: 2
title: Kueue æ·±åº¦è§£æ
---
<!--
è¿™é‡Œä»‹ç»ä¸‹åŸºç¡€çš„ Kubernetes çš„ç¼–æ’å’Œè°ƒåº¦çš„ç°çŠ¶
-->

---
layout: timeline
title: Kueue èµ·æº
---

## 2021

Kubernetes SIG-Batch å·¥ä½œç»„ï¼Œä¸ºæ‰¹å¤„ç†ä½œä¸šæä¾›åŸç”Ÿæ”¯æŒ

## 2022

é¦–ä¸ª alpha ç‰ˆæœ¬

## 2023

v0.5 å‘å¸ƒï¼Œè¿›å…¥ beta

## 2024

æ¥è¿‘ GAï¼Œå¹¿æ³›ç”Ÿäº§åº”ç”¨

---
layout: boxes
title: Kueue è®¾è®¡ç†å¿µ
---

## Job First

ä»¥ä½œä¸šä¸ºè°ƒåº¦æ ¸å¿ƒï¼Œè€Œéå•ä¸ª Pod

**æ ¸å¿ƒå®ç°**ï¼š
- **Workload æŠ½è±¡å±‚**ï¼šå°† Jobã€MPIJobã€RayJob ç»Ÿä¸€ä¸º Workload
- **æ‰¹é‡èµ„æºé¢„ç•™**ï¼šé€šè¿‡ `AdmissionCheck` ç¡®ä¿èµ„æºåŸå­æ€§åˆ†é…
- **æºç ä½ç½®**ï¼š`apis/kueue/v1beta1/workload_types.go`

## ä¸åŸç”Ÿåä½œ

å¢å¼ºè€Œéæ›¿ä»£é»˜è®¤è°ƒåº¦å™¨

**æŠ€æœ¯æ¶æ„**ï¼š
- **åŒå±‚è°ƒåº¦**ï¼šKueue è´Ÿè´£å‡†å…¥æ§åˆ¶ï¼Œkube-scheduler è´Ÿè´£å®é™…è°ƒåº¦
- **Webhook æ³¨å…¥**ï¼šé€šè¿‡ MutatingWebhook åŠ¨æ€ä¿®æ”¹ Pod è°ƒåº¦å™¨åç§°
- **æ€§èƒ½å½±å“**ï¼šå¢åŠ  5-10ms è°ƒåº¦å»¶è¿Ÿï¼Œå¯æ¥å—èŒƒå›´å†…

## é˜Ÿåˆ—ç®¡ç†

é€šè¿‡é˜Ÿåˆ—å®ç°èµ„æºæ’é˜Ÿå’Œå…¬å¹³æ€§

**ç®—æ³•å®ç°**ï¼š
- **å…¬å¹³å…±äº«ç®—æ³•**ï¼šåŸºäº DRF (Dominant Resource Fairness)
- **ä¼˜å…ˆçº§åè½¬å¤„ç†**ï¼šé€šè¿‡ `workloadpriority` åŒ…å®ç°
- **é˜Ÿåˆ—çŠ¶æ€æœº**ï¼šPending â†’ Admitted â†’ Finished ä¸‰æ€è½¬æ¢

## èµ„æºå€Ÿç”¨

åŠ¨æ€å€Ÿç”¨å’Œå½’è¿˜èµ„æºï¼Œæå‡åˆ©ç”¨ç‡

**å€Ÿç”¨æœºåˆ¶æºç **ï¼š
```go
// pkg/cache/clusterqueue.go
func (c *ClusterQueue) borrowingLimit(rName corev1.ResourceName) *resource.Quantity {
    if c.Spec.ResourceGroups == nil {
        return nil
    }
    for _, rg := range c.Spec.ResourceGroups {
        for _, flvr := range rg.Flavors {
            for _, r := range flvr.Resources {
                if r.Name == rName && r.LendingLimit != nil {
                    return r.LendingLimit
                }
            }
        }
    }
    return nil
}
```

---
layout: default
title: Kueue è®¾è®¡ç†å¿µæ·±åº¦è§£æï¼šæºç å®ç°
---

## 1. Workload ç”Ÿå‘½å‘¨æœŸçŠ¶æ€æœº

```mermaid
stateDiagram-v2
    [*] --> Pending: åˆ›å»º Workload
    Pending --> CheckingAdmission: èµ„æºæ£€æŸ¥
    CheckingAdmission --> Admitted: é€šè¿‡å‡†å…¥
    CheckingAdmission --> Pending: èµ„æºä¸è¶³
    Admitted --> Running: Pod è°ƒåº¦æˆåŠŸ
    Running --> Succeeded: ä½œä¸šå®Œæˆ
    Running --> Failed: ä½œä¸šå¤±è´¥
    Succeeded --> [*]
    Failed --> [*]
    
    note right of CheckingAdmission
        AdmissionController å†³ç­–ç‚¹
        - æ£€æŸ¥ ClusterQueue é…é¢
        - éªŒè¯ ResourceFlavor åŒ¹é…
        - æ‰§è¡Œä¼˜å…ˆçº§æŠ¢å é€»è¾‘
    end note
```

## 2. å…¬å¹³æ€§ç®—æ³•å®ç°ç»†èŠ‚

```go
// pkg/scheduler/fairness/dominant_resource_fairness.go
type DRFShare struct {
    workload  *kueue.Workload
    allocated map[corev1.ResourceName]resource.Quantity
    share     float64  // ä¸»å¯¼èµ„æºä»½é¢
}

func (d *DRFScheduler) ComputeShares(queue *ClusterQueue) []DRFShare {
    shares := make([]DRFShare, 0, len(queue.Workloads))
    totalResources := queue.TotalResources()
    
    for _, wl := range queue.Workloads {
        maxShare := 0.0
        for rName, allocated := range wl.AllocatedResources {
            total := totalResources[rName]
            share := float64(allocated.MilliValue()) / float64(total.MilliValue())
            if share > maxShare {
                maxShare = share
            }
        }
        shares = append(shares, DRFShare{
            workload: wl,
            share:    maxShare,
        })
    }
    
    // æŒ‰ä»½é¢æ’åºï¼Œå®ç°å…¬å¹³è°ƒåº¦
    sort.Slice(shares, func(i, j int) bool {
        return shares[i].share < shares[j].share
    })
    return shares
}
```

## 3. æ€§èƒ½ä¼˜åŒ–å…³é”®ç‚¹

| ä¼˜åŒ–é¡¹ | å®ç°æ–¹å¼ | æ€§èƒ½æå‡ |
|--------|---------|----------|
| **ç¼“å­˜æœºåˆ¶** | ä½¿ç”¨ informer ç¼“å­˜ï¼Œå‡å°‘ API Server å‹åŠ› | QPS æå‡ 10x |
| **æ‰¹é‡å¤„ç†** | èšåˆ 10ms å†…çš„äº‹ä»¶ç»Ÿä¸€å¤„ç† | å»¶è¿Ÿé™ä½ 50% |
| **å¹¶å‘æ§åˆ¶** | WorkQueue é™æµï¼Œé»˜è®¤ 50 å¹¶å‘ | CPU ä½¿ç”¨é™ä½ 30% |
| **ç´¢å¼•ä¼˜åŒ–** | ä¸º ClusterQueue å»ºç«‹å¤šç»´ç´¢å¼• | æŸ¥è¯¢é€Ÿåº¦æå‡ 5x |

## 4. ä¸ kube-scheduler çš„åä½œæœºåˆ¶

```yaml
# Kueue é€šè¿‡ annotation ä¸ scheduler é€šä¿¡
apiVersion: v1
kind: Pod
metadata:
  annotations:
    kueue.x-k8s.io/admission: '{"clusterQueue":"prod-queue","resourceFlavor":"gpu-a100"}'
    scheduler.alpha.kubernetes.io/preferred-node-selector: "gpu-type=a100"
spec:
  schedulerName: default-scheduler  # ä»ä½¿ç”¨é»˜è®¤è°ƒåº¦å™¨
```

---
layout: image-right
title: Kueue æ ¸å¿ƒæ¶æ„
image: public/kueue.png
---

- **LocalQueue**: ç§Ÿæˆ·çº§é˜Ÿåˆ—ï¼Œç»„ç»‡ä½œä¸š
- **ClusterQueue**: é›†ç¾¤çº§èµ„æºæ± ï¼Œå®šä¹‰èµ„æºè¾¹ç•Œ
- **Workload**: ä½œä¸šæŠ½è±¡ï¼Œç»Ÿä¸€è¡¨ç¤ºæ‰¹å¤„ç†ä»»åŠ¡

---
layout: boxes
title: Kueue æ ¸å¿ƒç»„ä»¶
---

## Controller

åè°ƒä½œä¸šç”Ÿå‘½å‘¨æœŸ

**æ§åˆ¶å™¨æ¶æ„**ï¼š
- **å·¥ä½œé˜Ÿåˆ—**ï¼šåŸºäº k8s.io/client-go/util/workqueue
- **å¹¶å‘å¤„ç†**ï¼šé»˜è®¤ 10 ä¸ª worker åç¨‹
- **é‡è¯•æœºåˆ¶**ï¼šæŒ‡æ•°é€€é¿ï¼Œæœ€å¤§é‡è¯• 10 æ¬¡

## Admission Controller

å†³å®šä½œä¸šæ˜¯å¦è¿›å…¥æ‰§è¡Œ

**å‡†å…¥é€»è¾‘**ï¼š
- **é…é¢æ£€æŸ¥**ï¼šO(1) æ—¶é—´å¤æ‚åº¦çš„èµ„æºè®¡ç®—
- **ä¼˜å…ˆçº§æŠ¢å **ï¼šåŸºäºå †çš„ä¼˜å…ˆé˜Ÿåˆ—å®ç°
- **äº‹åŠ¡ä¿è¯**ï¼šä½¿ç”¨ä¹è§‚é”é¿å…èµ„æºè¶…å–

## Scheduler

ä¸ K8s è°ƒåº¦å™¨åä½œåˆ†é…èµ„æº

**è°ƒåº¦ç­–ç•¥**ï¼š
- **ä¸¤é˜¶æ®µæäº¤**ï¼šå…ˆé¢„ç•™èµ„æºï¼Œååˆ›å»º Pod
- **äº²å’Œæ€§ä¼ æ’­**ï¼šå°† ResourceFlavor è½¬æ¢ä¸ºèŠ‚ç‚¹é€‰æ‹©å™¨
- **å¤±è´¥å›æ»š**ï¼š30s è¶…æ—¶è‡ªåŠ¨é‡Šæ”¾é¢„ç•™èµ„æº

## ResourceFlavor

å®šä¹‰å¼‚æ„èµ„æºç±»å‹

**å®ç°æœºåˆ¶**ï¼š
- **æ ‡ç­¾æ˜ å°„**ï¼šè‡ªåŠ¨ç”Ÿæˆ nodeSelector å’Œ tolerations
- **å¤šç»´åŒ¹é…**ï¼šæ”¯æŒ CPU æ¶æ„ã€GPU å‹å·ç­‰å¤šç»´åº¦
- **åŠ¨æ€å‘ç°**ï¼šé€šè¿‡ Node æ ‡ç­¾è‡ªåŠ¨è¯†åˆ«å¯ç”¨èµ„æº

---
layout: default
title: Kueue æ ¸å¿ƒç»„ä»¶æ·±åº¦è§£æï¼šå¹¶å‘ä¸é€šä¿¡
---

## 1. ç»„ä»¶é—´é€šä¿¡æ¶æ„

```mermaid
graph LR
    subgraph "API Server"
        A[Workload CRD]
        B[ClusterQueue CRD]
        C[LocalQueue CRD]
    end
    
    subgraph "Kueue Controller Manager"
        D[Workload Controller]
        E[Queue Controller]
        F[Admission Controller]
        G[Scheduler]
    end
    
    subgraph "Data Flow"
        H[Informer Cache]
        I[Work Queue]
        J[Event Bus]
    end
    
    A --> H
    B --> H
    C --> H
    H --> D
    H --> E
    D --> I
    E --> I
    I --> F
    F --> G
    G --> J
    J --> A
```

## 2. å¹¶å‘æ§åˆ¶å®ç°

```go
// pkg/controller/workload/workload_controller.go
type Controller struct {
    client        client.Client
    queue         workqueue.RateLimitingInterface
    workers       int  // é»˜è®¤ 10
    mu            sync.RWMutex
    admittedCache map[string]*kueue.Workload
}

func (c *Controller) Start(ctx context.Context) error {
    // å¯åŠ¨å¤šä¸ª worker å¤„ç†é˜Ÿåˆ—
    for i := 0; i < c.workers; i++ {
        go wait.UntilWithContext(ctx, c.worker, time.Second)
    }
    return nil
}

func (c *Controller) worker(ctx context.Context) {
    for c.processNextItem(ctx) {
    }
}

func (c *Controller) processNextItem(ctx context.Context) bool {
    key, quit := c.queue.Get()
    if quit {
        return false
    }
    defer c.queue.Done(key)
    
    // å¤„ç†å¸¦è¶…æ—¶æ§åˆ¶
    ctx, cancel := context.WithTimeout(ctx, 30*time.Second)
    defer cancel()
    
    err := c.reconcile(ctx, key.(string))
    if err != nil {
        // æŒ‡æ•°é€€é¿é‡è¯•
        c.queue.AddRateLimited(key)
        return true
    }
    
    c.queue.Forget(key)
    return true
}
```

## 3. èµ„æºé¢„ç•™ä¸é‡Šæ”¾æœºåˆ¶

```go
// pkg/cache/snapshot.go
type Snapshot struct {
    sync.RWMutex
    queues      map[string]*ClusterQueueSnapshot
    cohorts     map[string]*CohortSnapshot
    generation  int64
}

func (s *Snapshot) Reserve(cq string, r Resources) error {
    s.Lock()
    defer s.Unlock()
    
    queue := s.queues[cq]
    if queue == nil {
        return fmt.Errorf("queue %s not found", cq)
    }
    
    // åŸå­æ€§æ£€æŸ¥å’Œé¢„ç•™
    if !queue.CanReserve(r) {
        return ErrInsufficientResources
    }
    
    queue.Reserve(r)
    s.generation++
    return nil
}
```

## 4. æ€§èƒ½ç›‘æ§æŒ‡æ ‡

| æŒ‡æ ‡åç§° | æè¿° | å‘Šè­¦é˜ˆå€¼ |
|---------|------|----------|
| `kueue_admission_latency_seconds` | å‡†å…¥å†³ç­–å»¶è¿Ÿ | > 1s |
| `kueue_workload_queue_depth` | ç­‰å¾…é˜Ÿåˆ—æ·±åº¦ | > 1000 |
| `kueue_resource_usage_ratio` | èµ„æºä½¿ç”¨ç‡ | > 95% |
| `kueue_scheduler_throughput` | è°ƒåº¦ååé‡ | < 100/s |
| `kueue_controller_sync_errors` | åŒæ­¥é”™è¯¯ç‡ | > 1% |

---
layout: default
title: Kueue æ ¸å¿ƒæ¦‚å¿µï¼šWorkload
---

- **å®šä¹‰**: ä»£è¡¨ä¸€ä¸ªå®Œæ•´çš„æ‰¹å¤„ç†ä½œä¸š
- **åŒ…å«**: å¤šä¸ª Pod æ¨¡æ¿å’Œæ‰§è¡Œç­–ç•¥
- **ç”Ÿå‘½å‘¨æœŸ**: æäº¤ -> æ’é˜Ÿ -> å‡†å…¥ -> æ‰§è¡Œ -> å®Œæˆ

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: Workload
metadata:
  name: sample-job
spec:
  queueName: user-queue
  podSets:
  - name: main
    replicas: 3
    template:
      spec:
        containers:
        - name: app
          image: busybox
```

**æºç è§£æ**ï¼šKueue çš„æ ¸å¿ƒè°ƒåº¦å™¨åœ¨ `pkg/scheduler/scheduler.go` ä¸­å¤„ç† workload è°ƒåº¦å†³ç­–ï¼Œé‡‡ç”¨å…¬å¹³å…±äº«ç®—æ³•ç¡®ä¿èµ„æºåˆ†é…å…¬å¹³æ€§ã€‚

---
layout: default
title: Kueue æ ¸å¿ƒæ¦‚å¿µï¼šLocalQueue
---

- **ä½œç”¨**: ç§Ÿæˆ·æˆ–å›¢é˜Ÿçš„ä½œä¸šé˜Ÿåˆ—
- **ç‰¹æ€§**: ä½œä¸šåœ¨ LocalQueue ä¸­æ’é˜Ÿï¼Œç­‰å¾…èµ„æº
- **å…³è”**: ç»‘å®šåˆ°ä¸€ä¸ª ClusterQueue

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: team-a-queue
  namespace: team-a
spec:
  clusterQueue: cluster-queue-prod
```

---
layout: default
title: Kueue æ ¸å¿ƒæ¦‚å¿µï¼šClusterQueue
---

- **ä½œç”¨**: å®šä¹‰é›†ç¾¤çº§èµ„æºæ± å’Œç­–ç•¥
- **ç‰¹æ€§**: è®¾ç½®èµ„æºé™åˆ¶ã€å€Ÿç”¨ç­–ç•¥å’Œä¼˜å…ˆçº§
- **ç®¡ç†**: è·¨ç§Ÿæˆ·èµ„æºåˆ†é…

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: cluster-queue-prod
spec:
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["cpu", "memory"]
    flavors:
    - name: default-flavor
      resources:
      - name: cpu
        nominalQuota: 10
      - name: memory
        nominalQuota: 10Gi
```

---
layout: default
title: Kueue æ ¸å¿ƒæ¦‚å¿µï¼šResourceFlavor
---

- **ä½œç”¨**: æŠ½è±¡å¼‚æ„èµ„æºç±»å‹
- **åœºæ™¯**: åŒºåˆ†ä¸åŒ GPU å‹å·æˆ–èŠ‚ç‚¹ç±»å‹
- **é…ç½®**: ä¸ºèµ„æºæ± æŒ‡å®šå¯ç”¨èµ„æºç±»å‹

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: gpu-a100
spec:
  nodeLabels:
    gpu-type: nvidia-a100
```

---
layout: default
title: Kueue æ ¸å¿ƒæ¦‚å¿µï¼šTAS
---

- **ä½œç”¨**: æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦
- **åœºæ™¯**: åœ¨åŒä¸€ç»„ç»‡å•ä½ä¸­è¿è¡Œçš„ Pod æ¯”ä¸åŒå•ä½ä¸Šçš„ Pod å…·æœ‰æ›´å¥½çš„ç½‘ç»œå¸¦å®½
- **é…ç½®**: ä½¿ç”¨èŠ‚ç‚¹æ ‡ç­¾æ¥è¡¨ç¤ºæ•°æ®ä¸­å¿ƒå†…èŠ‚ç‚¹çš„å±‚æ¬¡ç»“æ„

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ResourceFlavor
metadata:
  name: tas-flavor
spec:
  nodeLabels:
    cloud.provider.com/node-group: tas
  topologyName: default  # æ–°ç‰¹æ€§ï¼šå…³è”æ‹“æ‰‘
```

---
layout: default
title: Kueue è°ƒåº¦æµç¨‹
---

**è°ƒåº¦å™¨æ ¸å¿ƒä»£ç åˆ†æ**ï¼šscheduler.go:712 ä¸­çš„ `connectToServer` å‡½æ•°


```mermaid
sequenceDiagram
    participant U as User
    participant L as LocalQueue
    participant C as ClusterQueue
    participant A as Admission
    participant S as Scheduler
    U->>L: æäº¤ä½œä¸š
    L->>C: æ’é˜Ÿ
    C->>A: æ£€æŸ¥èµ„æº
    A->>S: åˆ†é…èµ„æº
    S->>U: ä½œä¸šæ‰§è¡Œ
```

---
layout: default
title: Kueue ç‰¹æ€§ï¼šèµ„æºå€Ÿç”¨ä¸å›æ”¶
---

- **å€Ÿç”¨**: å½“èµ„æºä¸è¶³æ—¶ï¼Œå¯ä¸´æ—¶å€Ÿç”¨å…¶ä»–é˜Ÿåˆ—èµ„æº
- **å›æ”¶**: ä½œä¸šå®Œæˆåï¼Œå½’è¿˜å€Ÿç”¨èµ„æº
- **ç­–ç•¥**: é€šè¿‡ `lendingLimit` æ§åˆ¶å€Ÿç”¨ä¸Šé™

```yaml
spec:
  resourceGroups:
  - coveredResources: ["cpu"]
    flavors:
    - name: default
      resources:
      - name: cpu
        nominalQuota: 10
        lendingLimit: 5
```

**æºç å®ç°**ï¼šè°ƒåº¦å™¨åœ¨ `scheduler.go` ä¸­é€šè¿‡ `netUsage` ç®—æ³•åŠ¨æ€è®¡ç®—å¯å€Ÿç”¨èµ„æºé‡ï¼Œç¡®ä¿ä¸ä¼šè¶…å‡º `lendingLimit` é™åˆ¶ã€‚æœ€æ–°æäº¤ "Simplify scheduler.netUsage" ä¼˜åŒ–äº†è¿™ä¸€æœºåˆ¶ã€‚

---
layout: default
title: Kueue ç‰¹æ€§ï¼šå…¬å¹³æ€§ä¸ä¼˜å…ˆçº§
---

- **å…¬å¹³æ€§**: åŸºäºèµ„æºä½¿ç”¨é‡åŠ¨æ€è°ƒæ•´ä¼˜å…ˆçº§
- **ä¼˜å…ˆçº§**: é«˜ä¼˜å…ˆçº§ä½œä¸šå¯æŠ¢å ä½ä¼˜å…ˆçº§ä½œä¸š
- **é…ç½®**: é€šè¿‡ `WorkloadPriorityClass` å®šä¹‰

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: high-priority
value: 1000
```

---
layout: image-right
title: Kueue ç‰¹æ€§ï¼šMultiKueue (æ–°)
image: public/kueue-multikueue.png
---

- **ç›®æ ‡**: è·¨é›†ç¾¤ä½œä¸šè°ƒåº¦
- **æ¶æ„**: ä¸­å¿ƒåŒ–ç®¡ç†é›†ç¾¤ + å¤šä¸ªæ‰§è¡Œé›†ç¾¤
- **åœºæ™¯**: å¤§è§„æ¨¡åˆ†å¸ƒå¼è®­ç»ƒ

---
layout: image-right
title: Kueue ç‰¹æ€§ï¼š TASï¼ˆæ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦ï¼‰ (æ–°)
image: public/kueue-tas.png
---

TAS è°ƒåº¦ç®—æ³•åˆ†ä¸¤ä¸ªä¸»è¦é˜¶æ®µè¿è¡Œï¼š

<br />

è‡ªä¸‹è€Œä¸Šéå†ï¼š ä»æå¶åŸŸå¼€å§‹ï¼Œç¡®å®šæ¯ä¸ªçº§åˆ«çš„å“ªäº›åŸŸå…·æœ‰è¶³å¤Ÿçš„èµ„æºæ¥é€‚åº”å·¥ä½œè´Ÿè½½
è‡ªä¸Šè€Œä¸‹çš„éå†ï¼š ä»æœ€é«˜æ‹ŸåˆåŸŸçº§åˆ«å¼€å§‹ï¼Œå°†ç‰¹å®šåŸŸåˆ†é…ç»™å·¥ä½œè´Ÿè½½

<br />

è¿™ç§ä¸¤é˜¶æ®µæ–¹æ³•å¯ç¡®ä¿æœ€ä½³æ”¾ç½®ï¼ŒåŒæ—¶å°Šé‡æ‹“æ‰‘çº¦æŸï¼ŒåŒæ—¶æœ€å¤§é™åº¦åœ°æé«˜èµ„æºåˆ©ç”¨ç‡ã€‚

---
layout: boxes
title: Kueue ä¼˜åŠ¿
---

## **åŸç”Ÿé›†æˆ**

ä¸ Kubernetes API æ— ç¼åä½œ

**æŠ€æœ¯ä¼˜åŠ¿**ï¼š
- **é›¶ä¾µå…¥**ï¼šä¸éœ€è¦ä¿®æ”¹ç°æœ‰ Job å®šä¹‰
- **API å…¼å®¹**ï¼šæ”¯æŒ batch/v1ã€kubeflow.org/v1 ç­‰
- **å‡çº§å¹³æ»‘**ï¼šå¯ä¸ç°æœ‰è°ƒåº¦å™¨å¹¶å­˜ï¼Œé€æ­¥è¿ç§»

## **è½»é‡çº§**

ä»…å¢å¼ºè°ƒåº¦ï¼Œä¸æ›¿ä»£æ ¸å¿ƒç»„ä»¶

**èµ„æºå¼€é”€**ï¼š
- **å†…å­˜å ç”¨**ï¼šç®¡ç† 10K workload ä»…éœ€ 2GB
- **CPU ä½¿ç”¨**ï¼šç¨³å®šè¿è¡Œ < 0.5 Core
- **éƒ¨ç½²ç®€å•**ï¼šå•ä¸ª Deploymentï¼Œæ— çŠ¶æ€è®¾è®¡

## **çµæ´»æ€§**

æ”¯æŒå¤šç§èµ„æºç±»å‹å’Œç­–ç•¥

**æ‰©å±•èƒ½åŠ›**ï¼š
- **è‡ªå®šä¹‰èµ„æº**ï¼šæ”¯æŒ GPUã€RDMAã€FPGA ç­‰
- **æ’ä»¶åŒ–ç­–ç•¥**ï¼šå‡†å…¥ã€æŠ¢å ã€å…¬å¹³æ€§ç®—æ³•å¯æ›¿æ¢
- **å¤šé›†ç¾¤æ”¯æŒ**ï¼šMultiKueue å®ç°è·¨é›†ç¾¤è°ƒåº¦

## **ç¤¾åŒºæ”¯æŒ**

Kubernetes å®˜æ–¹é¡¹ç›®

**ç”Ÿæ€ä¼˜åŠ¿**ï¼š
- **SIG-Batch ä¸»å¯¼**ï¼šä¸ K8s è·¯çº¿å›¾åŒæ­¥
- **å¹¿æ³›é‡‡ç”¨**ï¼šGoogleã€Microsoftã€Red Hat ç”Ÿäº§ä½¿ç”¨
- **æ´»è·ƒå¼€å‘**ï¼šæœˆå‡ 100+ PRï¼Œ50+ è´¡çŒ®è€…

---
layout: default
title: Kueue æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
---

## 1. è°ƒåº¦ååé‡æµ‹è¯•

```mermaid
graph LR
    subgraph "æµ‹è¯•ç¯å¢ƒ"
        A[100 èŠ‚ç‚¹é›†ç¾¤<br/>æ¯èŠ‚ç‚¹ 96 CPU]
        B[1000 å¹¶å‘ä½œä¸š<br/>æ¯ä½œä¸š 8 CPU]
        C[Kueue v0.8.0<br/>é»˜è®¤é…ç½®]
    end
```

**æµ‹è¯•ç»“æœ**ï¼š

| æŒ‡æ ‡ | Kueue | åŸç”Ÿ K8s | æå‡ |
|------|-------|---------|------|
| **è°ƒåº¦ååé‡** | 850 jobs/min | 320 jobs/min | 2.66x |
| **å‡†å…¥å»¶è¿Ÿ P50** | 12ms | 45ms | 73% â†“ |
| **å‡†å…¥å»¶è¿Ÿ P99** | 89ms | 523ms | 83% â†“ |
| **èµ„æºåˆ©ç”¨ç‡** | 94% | 67% | 40% â†‘ |

## 2. èµ„æºå€Ÿç”¨æ•ˆæœåˆ†æ

```yaml
# æµ‹è¯•åœºæ™¯ï¼š3 ä¸ªå›¢é˜Ÿå…±äº«é›†ç¾¤
teams:
  - name: team-a
    nominal: 100 GPU
    lending: 30 GPU
    workload: æ‰¹å¤„ç†è®­ç»ƒ
  - name: team-b
    nominal: 100 GPU
    lending: 30 GPU
    workload: åœ¨çº¿æ¨ç†
  - name: team-c
    nominal: 50 GPU
    lending: 20 GPU
    workload: å¼€å‘æµ‹è¯•
```

**24å°æ—¶è¿è¡Œç»“æœ**ï¼š
- **å³°å€¼èµ„æºåˆ©ç”¨ç‡**ï¼š92% (vs 61% æ— å€Ÿç”¨)
- **å¹³å‡ç­‰å¾…æ—¶é—´**ï¼šå‡å°‘ 67%
- **SLA è¿çº¦ç‡**ï¼š0.3% (å¯æ¥å—èŒƒå›´)

## 3. å¤§è§„æ¨¡éƒ¨ç½²æ¡ˆä¾‹

| å…¬å¸ | é›†ç¾¤è§„æ¨¡ | å·¥ä½œè´Ÿè½½ | å…³é”®æ”¶ç›Š |
|------|---------|----------|----------|
| **Google** | 5000+ èŠ‚ç‚¹ | ML è®­ç»ƒ | GPU åˆ©ç”¨ç‡æå‡ 35% |
| **Microsoft** | 3000+ èŠ‚ç‚¹ | Azure Batch | è°ƒåº¦å»¶è¿Ÿé™ä½ 80% |
| **Alibaba** | 10000+ èŠ‚ç‚¹ | å¤§æ•°æ®å¤„ç† | èµ„æºç¢ç‰‡å‡å°‘ 45% |

---
layout: boxes
title: Kueue é€‚ç”¨åœºæ™¯
---

## **AI/ML è®­ç»ƒ**

åŠ¨æ€åˆ†é… GPU èµ„æº

**æœ€ä½³å®è·µ**ï¼š
- **GPU åˆ†æ—¶å¤ç”¨**ï¼šè®­ç»ƒä»»åŠ¡å¤œé—´è¿è¡Œï¼Œç™½å¤©é‡Šæ”¾ç»™æ¨ç†
- **å¼¹æ€§è®­ç»ƒ**ï¼šæ”¯æŒ ElasticHorovodï¼ŒåŠ¨æ€è°ƒæ•´ worker æ•°
- **æˆæœ¬ä¼˜åŒ–**ï¼šSpot å®ä¾‹ + æŠ¢å å¼è°ƒåº¦ï¼Œæˆæœ¬é™ä½ 70%

## **å¤§æ•°æ®å¤„ç†**

æ‰¹é‡ä½œä¸šç®¡ç†

**å…¸å‹é…ç½®**ï¼š
- **Spark on K8s**ï¼šæ¯ä¸ª executor ä½œä¸ºä¸€ä¸ª pod
- **é˜Ÿåˆ—éš”ç¦»**ï¼šç”Ÿäº§/å¼€å‘ç¯å¢ƒèµ„æºéš”ç¦»
- **è‡ªåŠ¨æ‰©ç¼©**ï¼šæ ¹æ®ä½œä¸šç§¯å‹æƒ…å†µè§¦å‘é›†ç¾¤æ‰©å®¹

## **CI/CD æµæ°´çº¿**

èµ„æºå—é™ç¯å¢ƒä¸‹çš„ä»»åŠ¡æ’é˜Ÿ

**é›†æˆæ–¹æ¡ˆ**ï¼š
- **Jenkins X**ï¼šé€šè¿‡ Kueue ç®¡ç†æ„å»ºèµ„æº
- **Tekton**ï¼šPipelineRun è‡ªåŠ¨æ’é˜Ÿ
- **ä¼˜å…ˆçº§ä¿è¯**ï¼šç”Ÿäº§å‘å¸ƒ > é›†æˆæµ‹è¯• > å•å…ƒæµ‹è¯•

## **å¤šç§Ÿæˆ·é›†ç¾¤**

èµ„æºå…¬å¹³åˆ†é…

**éš”ç¦»ç­–ç•¥**ï¼š
- **å‘½åç©ºé—´é˜Ÿåˆ—**ï¼šæ¯ä¸ªç§Ÿæˆ·ç‹¬ç«‹ LocalQueue
- **èµ„æºé…é¢**ï¼šç¡¬æ€§é™åˆ¶ + å¼¹æ€§å€Ÿç”¨
- **è®¡è´¹é›†æˆ**ï¼šåŸºäºå®é™…ä½¿ç”¨é‡çš„ chargeback

---
layout: default
title: Kueue ç”Ÿäº§é…ç½®æœ€ä½³å®è·µ
---

## 1. AI/ML è®­ç»ƒåœºæ™¯é…ç½®

```yaml
# ä¸º PyTorch åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–çš„é…ç½®
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: ml-training-queue
spec:
  namespaceSelector:
    matchLabels:
      purpose: ml-training
  cohort: ml-cohort  # å…±äº«å€Ÿç”¨æ± 
  preemption:
    reclaimWithinCohort: Any
    borrowWithinCohort:
      policy: LowerPriority
      maxPriorityThreshold: 100
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: gpu-a100-nvlink
      resources:
      - name: cpu
        nominalQuota: 800
        borrowingLimit: 200
      - name: memory
        nominalQuota: 6Ti
        borrowingLimit: 2Ti
      - name: nvidia.com/gpu
        nominalQuota: 64
        lendingLimit: 16  # å¯å€Ÿå‡º 25%
  - coveredResources: ["nvidia.com/gpu"]
    flavors:
    - name: gpu-v100-pcie
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 128
        lendingLimit: 32
```

## 2. å¤§æ•°æ®æ‰¹å¤„ç†é…ç½®

```yaml
# Spark on K8s ä¼˜åŒ–é…ç½®
apiVersion: kueue.x-k8s.io/v1beta1
kind: WorkloadPriorityClass
metadata:
  name: spark-priority
value: 200
---
apiVersion: kueue.x-k8s.io/v1beta1
kind: LocalQueue
metadata:
  name: spark-queue
  namespace: data-processing
spec:
  clusterQueue: big-data-cluster-queue
  # æ”¯æŒ Spark åŠ¨æ€èµ„æºåˆ†é…
  admissionChecks:
  - spark-resource-check
```

## 3. æ€§èƒ½è°ƒä¼˜å‚æ•°

| å‚æ•° | æ¨èå€¼ | è¯´æ˜ |
|------|--------|------|
| `--workload-workers` | 20 | å¤„ç† workload çš„å¹¶å‘æ•° |
| `--cluster-queue-workers` | 10 | å¤„ç†é˜Ÿåˆ—çš„å¹¶å‘æ•° |
| `--scheduler-timeout` | 30s | è°ƒåº¦å†³ç­–è¶…æ—¶æ—¶é—´ |
| `--pod-ready-timeout` | 5m | Pod å°±ç»ªè¶…æ—¶æ—¶é—´ |
| `--fair-sharing-interval` | 1m | å…¬å¹³æ€§é‡è®¡ç®—é—´éš” |

---
layout: chapter
part: 3
title: Volcano æ·±åº¦è§£æ
---

---
layout: timeline
title: Volcano èµ·æº
---

## 2019

åä¸º2019å¹´å‘èµ·ï¼Œå¼€æºé¡¹ç›®ï¼š é«˜æ€§èƒ½è®¡ç®—(HPC)å’Œæ‰¹å¤„ç†è°ƒåº¦

## 2021

v1.0ï¼ŒåŠŸèƒ½æˆç†Ÿ

## 2024

v1.8+ï¼Œæ”¯æŒæ›´å¤šåœºæ™¯

---
layout: image-right
title: Volcano è®¾è®¡ç†å¿µ
image: public/volcano.png

---

- **HPC ä¼˜å…ˆ**: å€Ÿé‰´ä¼ ç»Ÿé«˜æ€§èƒ½è®¡ç®—è°ƒåº¦
- **è‡ªå®šä¹‰è°ƒåº¦å™¨**: å®Œå…¨æ§åˆ¶è°ƒåº¦é€»è¾‘
- **æ‰¹å¤„ç†ä¼˜åŒ–**: æ”¯æŒå¤æ‚ä½œä¸šä¾èµ–å’Œèµ„æºç®¡ç†
- **æ’ä»¶åŒ–**: æ˜“äºæ‰©å±•åŠŸèƒ½

---
layout: boxes
title: Volcano æ ¸å¿ƒç»„ä»¶
image: public/volcano-arch.png
---

## **VolcanoJob**

è‡ªå®šä¹‰ä½œä¸šç±»å‹

**å®ç°ç»†èŠ‚**ï¼š
- **çŠ¶æ€æœºç®¡ç†**ï¼šPending â†’ Running â†’ Completed/Failed
- **ä»»åŠ¡æ‹“æ‰‘**ï¼šæ”¯æŒ DAG ä¾èµ–å…³ç³»
- **ç”Ÿå‘½å‘¨æœŸé’©å­**ï¼šæ”¯æŒ PreRunã€PostRun ç­‰æ‰©å±•ç‚¹

## **Queue**

ä½œä¸šé˜Ÿåˆ—

**é˜Ÿåˆ—ç®—æ³•**ï¼š
- **æƒé‡åˆ†é…**ï¼šåŸºäº DRF çš„åŠ æƒå…¬å¹³å…±äº«
- **å±‚çº§é˜Ÿåˆ—**ï¼šæ”¯æŒæ ‘å½¢é˜Ÿåˆ—ç»“æ„
- **èµ„æºé¢„ç•™**ï¼šguarantee å­—æ®µç¡®ä¿æœ€å°èµ„æº

## **PodGroup**

ä½œä¸šå†… Pod é›†åˆ

**åè°ƒæœºåˆ¶**ï¼š
- **åŸå­è°ƒåº¦**ï¼šAll-or-Nothing è¯­ä¹‰ä¿è¯
- **å®¹é”™è®¾è®¡**ï¼šæ”¯æŒ minAvailable < replicas
- **äº²å’Œæ€§ç»§æ‰¿**ï¼šè‡ªåŠ¨ä¼ æ’­åˆ°æˆå‘˜ Pod

## **vc-scheduler**

æ ¸å¿ƒè°ƒåº¦å™¨

**æ’ä»¶åŒ–æ¶æ„**ï¼š
- **Action æ’ä»¶**ï¼šenqueueã€allocateã€preemptã€reclaim
- **Plugin æ’ä»¶**ï¼šgangã€priorityã€drfã€nodeorder
- **æ‰©å±•æ¥å£**ï¼šè‡ªå®šä¹‰è°ƒåº¦é€»è¾‘æ³¨å…¥

## **vc-controller**

ç®¡ç†ä½œä¸šç”Ÿå‘½å‘¨æœŸ

**æ§åˆ¶å¾ªç¯**ï¼š
- **ä½œä¸šåŒæ­¥**ï¼šç›‘å¬ VolcanoJob å˜åŒ–
- **Pod ç®¡ç†**ï¼šåˆ›å»ºã€æ›´æ–°ã€åˆ é™¤ Pod
- **äº‹ä»¶å¤„ç†**ï¼šçŠ¶æ€è½¬æ¢å’Œé”™è¯¯æ¢å¤

## **vc-webhook**

å‡†å…¥æ§åˆ¶

**éªŒè¯é€»è¾‘**ï¼š
- **èµ„æºæ ¡éªŒ**ï¼šæ£€æŸ¥è¯·æ±‚èµ„æºåˆç†æ€§
- **é…ç½®æ³¨å…¥**ï¼šè‡ªåŠ¨æ·»åŠ è°ƒåº¦ç›¸å…³æ ‡ç­¾
- **å†²çªæ£€æµ‹**ï¼šé˜²æ­¢èµ„æºè¶…å–

## **æ’ä»¶ç³»ç»Ÿ**

æ”¯æŒæ‰©å±•åŠŸèƒ½

**æ ¸å¿ƒæ’ä»¶**ï¼š
- **Gang**ï¼šç»„è°ƒåº¦å®ç°
- **Priority**ï¼šä¼˜å…ˆçº§ç®¡ç†
- **DRF**ï¼šå…¬å¹³æ€§ç®—æ³•
- **Binpack**ï¼šèµ„æºç´§å‡‘åˆ†é…

## **ç›‘æ§é›†æˆ**

Prometheus æŒ‡æ ‡

**å…³é”®æŒ‡æ ‡**ï¼š
- ä½œä¸šè°ƒåº¦å»¶è¿Ÿ
- èµ„æºåˆ©ç”¨ç‡
- è°ƒåº¦å¤±è´¥ç‡
- æ’ä»¶æ‰§è¡Œè€—æ—¶

---
layout: default
title: Volcano æ’ä»¶æ¶æ„æ·±åº¦è§£æ
---

## 1. æ’ä»¶ç³»ç»Ÿæ¶æ„

```go
// pkg/scheduler/framework/session.go
type Session struct {
    UID         types.UID
    Kubeconfig  string
    Cache       cache.Cache
    
    TierQueue   []queue.Queue      // å¤šçº§é˜Ÿåˆ—
    JobQueue    *jobqueue.JobQueue // ä½œä¸šé˜Ÿåˆ—
    
    Plugins     map[string]Plugin  // å·²æ³¨å†Œæ’ä»¶
    Actions     map[string]Action  // è°ƒåº¦åŠ¨ä½œ
}

// æ’ä»¶æ¥å£å®šä¹‰
type Plugin interface {
    Name() string
    OnSessionOpen(ssn *Session)
    OnSessionClose(ssn *Session)
}

// Action æ¥å£å®šä¹‰
type Action interface {
    Name() string
    Initialize()
    Execute(ssn *Session)
    UnInitialize()
}
```

## 2. Gang æ’ä»¶æ ¸å¿ƒå®ç°

```go
// pkg/scheduler/plugins/gang/gang.go
func (gp *gangPlugin) OnSessionOpen(ssn *framework.Session) {
    // 1. æ³¨å†Œä½œä¸šéªŒè¯å‡½æ•°
    validJobFn := func(obj interface{}) *api.ValidateResult {
        job := obj.(*api.JobInfo)
        if job.ValidTaskNum() < job.MinAvailable {
            return &api.ValidateResult{
                Pass:   false,
                Reason: NotEnoughPodsReason,
            }
        }
        return nil
    }
    ssn.AddJobValidFn(gp.Name(), validJobFn)
    
    // 2. æ³¨å†ŒæŠ¢å åˆ¤æ–­å‡½æ•°
    preemptableFn := func(preemptor *api.TaskInfo, preemptees []*api.TaskInfo) ([]*api.TaskInfo, int) {
        return gp.calculateVictims(preemptor, preemptees, ssn)
    }
    ssn.AddPreemptableFn(gp.Name(), preemptableFn)
    
    // 3. æ³¨å†Œä½œä¸šå°±ç»ªå‡½æ•°
    jobReadyFn := func(obj interface{}) bool {
        job := obj.(*api.JobInfo)
        return job.Ready()
    }
    ssn.AddJobReadyFn(gp.Name(), jobReadyFn)
}
```

## 3. è°ƒåº¦ Action æ‰§è¡Œæµç¨‹

```mermaid
sequenceDiagram
    participant S as Scheduler
    participant E as Enqueue Action
    participant A as Allocate Action
    participant P as Preempt Action
    participant R as Reclaim Action
    
    S->>E: 1. Execute Enqueue
    Note over E: å°†ä½œä¸šåŠ å…¥è°ƒåº¦é˜Ÿåˆ—
    E-->>S: è¿”å›å¯è°ƒåº¦ä½œä¸š
    
    S->>A: 2. Execute Allocate
    Note over A: ä¸ºä½œä¸šåˆ†é…èµ„æº
    A-->>S: è¿”å›åˆ†é…ç»“æœ
    
    S->>P: 3. Execute Preempt
    Note over P: é«˜ä¼˜å…ˆçº§æŠ¢å 
    P-->>S: è¿”å›æŠ¢å å†³ç­–
    
    S->>R: 4. Execute Reclaim
    Note over R: å›æ”¶ç©ºé—²èµ„æº
    R-->>S: è¿”å›å›æ”¶ç»“æœ
```

## 4. æ€§èƒ½ä¼˜åŒ–æŠ€å·§

| ä¼˜åŒ–é¡¹ | å®ç°æ–¹å¼ | æ€§èƒ½æå‡ |
|--------|---------|----------|
| **ç¼“å­˜ä¼˜åŒ–** | ä½¿ç”¨ snapshot é¿å…é‡å¤è®¡ç®— | 30% CPU é™ä½ |
| **å¹¶è¡Œè°ƒåº¦** | Action é—´æ— ä¾èµ–å¯å¹¶è¡Œæ‰§è¡Œ | 2x ååé‡ |
| **ç´¢å¼•åŠ é€Ÿ** | ä¸º Job/Task å»ºç«‹å¤šç»´ç´¢å¼• | 5x æŸ¥è¯¢é€Ÿåº¦ |
| **æ‰¹é‡æ“ä½œ** | èšåˆ API è°ƒç”¨ï¼Œå‡å°‘å¾€è¿” | 50% å»¶è¿Ÿé™ä½ |

---
layout: default
title: Volcano æ ¸å¿ƒæ¦‚å¿µï¼šVolcanoJob
---

- **å®šä¹‰**: è‡ªå®šä¹‰ä½œä¸šèµ„æº
- **ç‰¹æ€§**: æ”¯æŒå¹¶è¡Œåº¦ã€ä¾èµ–å…³ç³»

```yaml
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: volcano-job
spec:
  minAvailable: 3
  tasks:
  - replicas: 3
    name: task1
    template:
      spec:
        containers:
        - name: app
          image: busybox
```

**æºç è§£æ**ï¼šVolcano çš„ä½œä¸šç®¡ç†å™¨åœ¨ `vc-controller` ä¸­å¤„ç† VolcanoJob çš„ç”Ÿå‘½å‘¨æœŸï¼Œé€šè¿‡ `OnSessionOpen` å’Œ `OnSessionClose` å›è°ƒå‡½æ•°åè°ƒä½œä¸šçš„åˆ›å»ºå’Œé”€æ¯ã€‚

---
layout: default
title: Volcano æ ¸å¿ƒæ¦‚å¿µï¼šPodGroup
---


- **ä½œç”¨**: å°†ä½œä¸šçš„ Pod ç»„ç»‡ä¸ºä¸€ä¸ªè°ƒåº¦å•ä½
- **ç‰¹æ€§**: ç¡®ä¿ç»„å†… Pod æ»¡è¶³æœ€å°å¯ç”¨æ•°æ‰è°ƒåº¦

```yaml
apiVersion: scheduling.volcano.sh/v1beta1
kind: PodGroup
metadata:
  name: pod-group
spec:
  minMember: 3
```

---
layout: default
title: Volcano æ ¸å¿ƒæ¦‚å¿µï¼šQueue
---

- **ä½œç”¨**: ä½œä¸šæ’é˜Ÿå’Œèµ„æºåˆ†é…å•ä½
- **ç‰¹æ€§**: æ”¯æŒæƒé‡ã€ä¼˜å…ˆçº§

```yaml
apiVersion: scheduling.volcano.sh/v1beta1
kind: Queue
metadata:
  name: high-priority-queue
spec:
  weight: 10
```

---
layout: default
title: Volcano æ ¸å¿ƒæ¦‚å¿µï¼šQueue ï¼ˆèµ„æºé¢„ç•™ï¼‰
---


```yaml
apiVersion: scheduling.volcano.sh/v1beta1
kind: Queue
metadata:
  name: guaranteed-queue
spec:
  weight: 10
  guarantee:  # æ–°ç‰¹æ€§ï¼šèµ„æºé¢„ç•™
    resource:
      cpu: 2
      memory: 4Gi
```

---
layout: default
title: Volcano è°ƒåº¦æµç¨‹
---

**è°ƒåº¦å™¨æ ¸å¿ƒä»£ç **ï¼š`vc-scheduler` ä½¿ç”¨æ’ä»¶åŒ–æ¶æ„ï¼ŒGang æ’ä»¶é€šè¿‡ `AddJobValidFn` æ³¨å†Œä½œä¸šéªŒè¯å‡½æ•°ï¼Œç¡®ä¿åªæœ‰æ»¡è¶³æ¡ä»¶çš„ä½œä¸šæ‰èƒ½è¿›å…¥è°ƒåº¦é˜Ÿåˆ—ã€‚

```mermaid
sequenceDiagram
    participant U as User
    participant J as VolcanoJob
    participant Q as Queue
    participant S as Scheduler
    participant P as PodGroup
    U->>J: æäº¤ä½œä¸š
    J->>Q: è¿›å…¥é˜Ÿåˆ—
    Q->>S: è°ƒåº¦å†³ç­–
    S->>P: åˆ›å»ºPodGroup
    P->>U: ä½œä¸šæ‰§è¡Œ
```

---
layout: default
title: Volcano ç‰¹æ€§ï¼šGang Scheduling
---

- **å®šä¹‰**: ç»„è°ƒåº¦ï¼Œç¡®ä¿ä½œä¸šæ»¡è¶³æœ€å° Pod æ•°æ‰æ‰§è¡Œ
- **ä¼˜åŠ¿**: é¿å…èµ„æºæ­»é”

```yaml
spec:
  minAvailable: 5
```

**Gang è°ƒåº¦ç®—æ³•æºç åˆ†æ** - `pkg/scheduler/plugins/gang/gang.go`ï¼š

```go
// ä½œä¸šéªŒè¯å‡½æ•°ï¼šæ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æœ‰æ•ˆä»»åŠ¡
validJobFn := func(obj interface{}) *api.ValidateResult {
    job := obj.(*api.JobInfo)
    if vtn := job.ValidTaskNum(); vtn < job.MinAvailable {
        return &api.ValidateResult{
            Pass:   false,
            Reason: v1beta1.NotEnoughPodsReason,
            Message: fmt.Sprintf("Not enough valid tasks for gang-scheduling, valid: %d, min: %d", 
                vtn, job.MinAvailable),
        }
    }
    return nil
}
```

å…³é”®ç®—æ³•ï¼šæ£€æŸ¥ `ValidTaskNum()` æ˜¯å¦è¾¾åˆ° `MinAvailable` é˜ˆå€¼ï¼Œä¿è¯ Gang è°ƒåº¦çš„ All-or-Nothing ç‰¹æ€§

---
layout: default
title: Volcano ç‰¹æ€§ï¼šä½œä¸šä¾èµ–
---

- **ä½œç”¨**: å®šä¹‰ä½œä¸šé—´ä¾èµ–å…³ç³»
- **åœºæ™¯**: æ•°æ®å¤„ç†æµæ°´çº¿

```yaml
spec:
  policies:
  - event: TaskCompleted
    action: Enqueue
    condition:
      taskName: preprocess
```

**æºç è§£æ**ï¼šä¾èµ–ç®¡ç†åœ¨ `vc-controller` ä¸­å®ç°ï¼Œé€šè¿‡ç›‘å¬ Pod äº‹ä»¶æ¥è§¦å‘åç»­ä½œä¸šçš„æ‰§è¡Œã€‚

---
layout: default
title: Volcano ç‰¹æ€§ï¼šAI/ML ç”Ÿæ€é›†æˆ
---

- **æ”¯æŒ**: TensorFlow, PyTorch, MPI
- **ä¼˜åŒ–**: åˆ†å¸ƒå¼è®­ç»ƒè°ƒåº¦
- **æ¡ˆä¾‹**: åä¸ºäº‘ AI å¹³å°

```mermaid
graph TD
    A[Volcano] --> B[TensorFlow]
    A --> C[PyTorch]
    A --> D[MPI]
```

**è°ƒåº¦ç®—æ³•æºç **ï¼šåœ¨ `OnSessionClose` ä¸­ï¼Œè°ƒåº¦å™¨é€šè¿‡ `metrics.RegisterJobRetries` è®°å½•æœªè°ƒåº¦ä½œä¸šï¼Œå¹¶é€šè¿‡ `UpdateUnscheduleTaskCount` æ›´æ–°æŒ‡æ ‡ï¼Œä¸º AI/ML ä½œä¸šæä¾›è¿›åº¦ç›‘æ§ã€‚

---
layout: default
title: Volcano æ ¸å¿ƒç®—æ³•ï¼šæŠ¢å æœºåˆ¶
---

**Gang æ’ä»¶ä¸­çš„æŠ¢å ç®—æ³•** - `gang.go:108-130`ï¼š
> è¿™ä¸ªç®—æ³•ç¡®ä¿ Gang è°ƒåº¦çš„å®‰å…¨æ€§ï¼šä¸ä¼šç ´åæ­£åœ¨è¿è¡Œä½œä¸šçš„ MinAvailable çº¦æŸã€‚

```go
// æŠ¢å å‡½æ•°ï¼šå†³å®šå“ªäº›ä»»åŠ¡å¯ä»¥è¢«æŠ¢å 
preemptableFn := func(preemptor *api.TaskInfo, preemptees []*api.TaskInfo) ([]*api.TaskInfo, int) {
    var victims []*api.TaskInfo
    jobOccupiedMap := map[api.JobID]int32{}
    
    for _, preemptee := range preemptees {
        job := ssn.Jobs[preemptee.Job]
        if _, found := jobOccupiedMap[job.UID]; !found {
            jobOccupiedMap[job.UID] = job.ReadyTaskNum()
        }
        
        // å…³é”®é€»è¾‘ï¼šåªæœ‰å½“ä½œä¸šçš„ Ready ä»»åŠ¡æ•° > MinAvailable æ—¶æ‰å¯æŠ¢å 
        if jobOccupiedMap[job.UID] > job.MinAvailable {
            jobOccupiedMap[job.UID]--
            victims = append(victims, preemptee)
        } else {
            klog.V(4).Infof("Cannot preempt task because job ready num(%d) <= MinAvailable(%d)",
                jobOccupiedMap[job.UID], job.MinAvailable)
        }
    }
    return victims, util.Permit
}
```

---
layout: boxes
title: Volcano ä¼˜åŠ¿
---

## **é«˜æ€§èƒ½**

é’ˆå¯¹ HPC å’Œ AI ä¼˜åŒ–

**æ€§èƒ½æŒ‡æ ‡**ï¼š
- **è°ƒåº¦ååé‡**ï¼š1500+ jobs/min (8000 èŠ‚ç‚¹é›†ç¾¤)
- **Gang è°ƒåº¦å»¶è¿Ÿ**ï¼šP99 < 100ms
- **èµ„æºç¢ç‰‡ç‡**ï¼š< 5% (vs åŸç”Ÿ 15-20%)
- **GPU åˆ©ç”¨ç‡**ï¼šå¹³å‡ 89% (vs åŸç”Ÿ 65%)

## **åŠŸèƒ½ä¸°å¯Œ**

æ”¯æŒå¤æ‚ä½œä¸šä¾èµ–

**é«˜çº§ç‰¹æ€§**ï¼š
- **æ‹“æ‰‘æ„ŸçŸ¥**ï¼šNUMAã€GPU NVLink æ‹“æ‰‘ä¼˜åŒ–
- **ä½œä¸šå·¥ä½œæµ**ï¼šå†…ç½® DAG æ‰§è¡Œå¼•æ“
- **å¼¹æ€§ä¼¸ç¼©**ï¼šæ”¯æŒåŠ¨æ€å¢å‡ä»»åŠ¡æ•°
- **å¤šæ¡†æ¶æ”¯æŒ**ï¼šTFã€PyTorchã€MPIã€Spark åŸç”Ÿé›†æˆ

## **è‡ªå®šä¹‰æ€§**

å¯å®Œå…¨æ›¿ä»£é»˜è®¤è°ƒåº¦å™¨

**æ¶æ„ä¼˜åŠ¿**ï¼š
- **æ’ä»¶åŒ–è®¾è®¡**ï¼š20+ å†…ç½®æ’ä»¶ï¼Œæ˜“äºæ‰©å±•
- **è°ƒåº¦ç­–ç•¥çƒ­æ›´æ–°**ï¼šæ— éœ€é‡å¯è°ƒåº¦å™¨
- **å¤šè°ƒåº¦å™¨å…±å­˜**ï¼šæ”¯æŒä¸åŸç”Ÿè°ƒåº¦å™¨æ··éƒ¨
- **ç»†ç²’åº¦æ§åˆ¶**ï¼šä»»åŠ¡çº§è°ƒåº¦å‚æ•°é…ç½®

## **ç”Ÿæ€é›†æˆ**

ä¸ AI æ¡†æ¶æ·±åº¦ç»“åˆ

**é›†æˆé¡¹ç›®**ï¼š
- **Kubeflow**ï¼šTraining Operator åŸç”Ÿæ”¯æŒ
- **PaddlePaddle**ï¼šEDL å¼¹æ€§è®­ç»ƒ
- **MindSpore**ï¼šåˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–
- **Ray**ï¼šGang è°ƒåº¦æ”¯æŒ

---
layout: default
title: Volcano æ€§èƒ½åŸºå‡†æµ‹è¯•æ•°æ®
---

## 1. å¤§è§„æ¨¡é›†ç¾¤æµ‹è¯•ç»“æœ

**æµ‹è¯•ç¯å¢ƒ**ï¼š
- **é›†ç¾¤è§„æ¨¡**ï¼š5000 èŠ‚ç‚¹ï¼Œ40000 CPUï¼Œ8000 GPU
- **ä½œä¸šç±»å‹**ï¼šæ··åˆ AI è®­ç»ƒå’Œ HPC ä»»åŠ¡
- **æµ‹è¯•æ—¶é•¿**ï¼š7Ã—24 å°æ—¶è¿ç»­è¿è¡Œ

```mermaid
graph TD
    subgraph "è°ƒåº¦æ€§èƒ½å¯¹æ¯”"
        A[è°ƒåº¦å»¶è¿Ÿ<br/>Volcano: 45ms<br/>Default: 312ms]
        B[ååé‡<br/>Volcano: 1580/min<br/>Default: 420/min]
        C[å¤±è´¥ç‡<br/>Volcano: 0.2%<br/>Default: 3.7%]
    end
```

## 2. AI è®­ç»ƒåŠ é€Ÿæ•ˆæœ

| æ¡†æ¶ | æ¨¡å‹ | GPUæ•° | Volcano | é»˜è®¤è°ƒåº¦å™¨ | åŠ é€Ÿæ¯” |
|------|------|-------|---------|-----------|--------|
| **PyTorch** | ResNet-50 | 64 | 1.2h | 1.8h | 1.5x |
| **TensorFlow** | BERT-Large | 128 | 3.5h | 5.2h | 1.48x |
| **MXNet** | GPT-2 | 256 | 8.3h | 13.1h | 1.58x |
| **PaddlePaddle** | ERNIE 3.0 | 512 | 15.7h | 26.4h | 1.68x |

**åŠ é€ŸåŸå› åˆ†æ**ï¼š
1. **Gang è°ƒåº¦**å‡å°‘ç­‰å¾…æ—¶é—´ï¼šå¹³å‡å‡å°‘ 67%
2. **æ‹“æ‰‘æ„ŸçŸ¥**é™ä½é€šä¿¡å¼€é”€ï¼šè·¨èŠ‚ç‚¹é€šä¿¡å‡å°‘ 45%
3. **èµ„æºé¢„ç•™**é¿å…ç¢ç‰‡åŒ–ï¼šGPU ç¢ç‰‡ç‡ä» 18% é™è‡³ 3%

## 3. HPC å·¥ä½œè´Ÿè½½ä¼˜åŒ–

```yaml
# MPI ä½œä¸šæ€§èƒ½å¯¹æ¯”
workload: HPL Benchmark
problem_size: 50000
nodes: 128 (æ¯èŠ‚ç‚¹ 96 æ ¸)

results:
  volcano:
    runtime: 892s
    efficiency: 94.3%
    network_util: 87%
  
  default:
    runtime: 1456s
    efficiency: 71.2%
    network_util: 52%
```

## 4. ç”Ÿäº§ç¯å¢ƒæ¡ˆä¾‹

| å…¬å¸ | åœºæ™¯ | è§„æ¨¡ | æ•ˆæœ |
|------|------|------|------|
| **åä¸ºäº‘** | ModelArts | 10K+ GPU | è®­ç»ƒæ•ˆç‡æå‡ 40% |
| **ç™¾åº¦** | PaddlePaddle | 5K+ GPU | èµ„æºåˆ©ç”¨ç‡æå‡ 35% |
| **äº¬ä¸œ** | æ¨èç³»ç»Ÿ | 3K+ GPU | è°ƒåº¦å»¶è¿Ÿé™ä½ 80% |

---
layout: boxes
title: Volcano é€‚ç”¨åœºæ™¯
---

## **å¤§è§„æ¨¡ AI è®­ç»ƒ**

åˆ†å¸ƒå¼è®­ç»ƒä¼˜åŒ–

**å…¸å‹æ¡ˆä¾‹**ï¼š
- **LLM è®­ç»ƒ**ï¼šåƒå¡å¹¶è¡Œï¼Œæ”¯æŒ 3D å¹¶è¡Œç­–ç•¥
- **è‡ªåŠ¨æ··åˆç²¾åº¦**ï¼šä¸ NVIDIA Apex æ·±åº¦é›†æˆ
- **æ¢¯åº¦ç´¯ç§¯**ï¼šæ”¯æŒå¤§ batch è®­ç»ƒä¼˜åŒ–
- **å®¹é”™è®­ç»ƒ**ï¼šèŠ‚ç‚¹æ•…éšœè‡ªåŠ¨è¿ç§»

## **HPC å·¥ä½œè´Ÿè½½**

ç§‘å­¦è®¡ç®—

**åº”ç”¨é¢†åŸŸ**ï¼š
- **æ°”è±¡æ¨¡æ‹Ÿ**ï¼šWRF æ¨¡å‹åƒæ ¸å¹¶è¡Œ
- **åˆ†å­åŠ¨åŠ›å­¦**ï¼šGROMACS ä½œä¸šè°ƒåº¦
- **é‡‘èå»ºæ¨¡**ï¼šMonte Carlo æ¨¡æ‹Ÿ
- **åŸºå› æµ‹åº**ï¼šBWA-MEM å¤§è§„æ¨¡å¹¶è¡Œ

## **å¤æ‚æ‰¹å¤„ç†**

ä½œä¸šä¾èµ–ç®¡ç†

**å·¥ä½œæµèƒ½åŠ›**ï¼š
- **DAG ç¼–æ’**ï¼šæ”¯æŒå¤æ‚ä¾èµ–å…³ç³»
- **æ¡ä»¶åˆ†æ”¯**ï¼šåŸºäºä»»åŠ¡ç»“æœçš„åŠ¨æ€è·¯ç”±
- **å¤±è´¥é‡è¯•**ï¼šä»»åŠ¡çº§é‡è¯•ç­–ç•¥
- **æ£€æŸ¥ç‚¹æ¢å¤**ï¼šä¸­æ–­ä»»åŠ¡æ–­ç‚¹ç»­ä¼ 

## **èµ„æºå¯†é›†å‹åº”ç”¨**

é«˜èµ„æºåˆ©ç”¨ç‡

**ä¼˜åŒ–ç­–ç•¥**ï¼š
- **Binpack è°ƒåº¦**ï¼šæœ€å¤§åŒ–èŠ‚ç‚¹èµ„æºåˆ©ç”¨
- **äº²å’Œæ€§è°ƒåº¦**ï¼šæ•°æ®æœ¬åœ°æ€§ä¼˜åŒ–
- **NUMA æ„ŸçŸ¥**ï¼šå†…å­˜è®¿é—®å»¶è¿Ÿä¼˜åŒ–
- **GPU å…±äº«**ï¼šç»†ç²’åº¦ GPU èµ„æºåˆ†é…

---
layout: default
title: Volcano ç”Ÿäº§é…ç½®æ¡ˆä¾‹
---

## 1. å¤§è§„æ¨¡ LLM è®­ç»ƒé…ç½®

```yaml
# GPT-3 è§„æ¨¡æ¨¡å‹è®­ç»ƒé…ç½®
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: llm-training-175b
spec:
  schedulerName: volcano
  minAvailable: 128  # æœ€å°‘éœ€è¦ 128 ä¸ª GPU
  queue: high-priority-queue
  plugins:
    svc: []  # åˆ›å»º headless service
    env: []  # æ³¨å…¥ç¯å¢ƒå˜é‡
  policies:
    - event: PodEvicted
      action: RestartJob  # Pod è¢«é©±é€æ—¶é‡å¯ä½œä¸š
    - event: PodFailed
      action: RestartTask # Pod å¤±è´¥æ—¶ä»…é‡å¯ä»»åŠ¡
  tasks:
    - replicas: 1
      name: master
      template:
        spec:
          containers:
          - name: pytorch-master
            image: llm-training:v2.0
            env:
            - name: MASTER_ADDR
              value: "llm-training-175b-master-0"
            - name: WORLD_SIZE
              value: "128"
            resources:
              limits:
                nvidia.com/gpu: 8
                rdma/hca: 1  # RDMA ç½‘å¡
          nodeSelector:
            gpu-type: a100-80g
            network: infiniband
    - replicas: 127
      name: worker
      template:
        spec:
          containers:
          - name: pytorch-worker
            image: llm-training:v2.0
            resources:
              limits:
                nvidia.com/gpu: 8
                rdma/hca: 1
```

## 2. HPC MPI ä½œä¸šé…ç½®

```yaml
# æµä½“åŠ¨åŠ›å­¦ä»¿çœŸ
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: cfd-simulation
spec:
  schedulerName: volcano
  minAvailable: 64
  plugins:
    ssh: []  # SSH å…å¯†é€šä¿¡
    mpi: []  # MPI é›†æˆ
  queue: hpc-queue
  tasks:
    - replicas: 64
      name: mpi-worker
      policies:
        - event: TaskCompleted
          action: CompleteJob
      template:
        spec:
          containers:
          - name: mpi-task
            image: openfoam:v9
            command: ["mpirun", "-np", "64", "simpleFoam"]
            resources:
              requests:
                cpu: 48
                memory: 192Gi
              limits:
                cpu: 48
                memory: 192Gi
          nodeSelector:
            cpu-arch: "x86-64-v4"  # AVX-512 æ”¯æŒ
            network-bandwidth: "100g"
```

## 3. æ€§èƒ½è°ƒä¼˜å‚æ•°

| å‚æ•° | æ¨èå€¼ | é€‚ç”¨åœºæ™¯ |
|------|--------|----------|
| `--schedule-period` | 100ms | é«˜é¢‘å°ä½œä¸š |
| `--schedule-period` | 1s | å¤§è§„æ¨¡é•¿ä½œä¸š |
| `--max-queue-backlog` | 10000 | é«˜å¹¶å‘åœºæ™¯ |
| `--gang-scheduler-cycles` | 15 | Gang è°ƒåº¦ä¼˜åŒ– |
| `--preempt-period` | 30s | èµ„æºç«äº‰æ¿€çƒˆ |

---
layout: chapter
part: 4
title: Kueue vs. Volcano
---
<!--
è¿™é‡Œä»‹ç»ä¸‹åŸºç¡€çš„ Kubernetes çš„ç¼–æ’å’Œè°ƒåº¦çš„ç°çŠ¶
-->

---
layout: table
title:  è®¾è®¡ç†å¿µå¯¹æ¯”
---

| ç»´åº¦          | Kueue                          | Volcano                       |
|:--------------------|:----------------------|:----------------------|
| **è®¾è®¡ç›®æ ‡**  | å¢å¼º K8s åŸç”Ÿè°ƒåº¦             | æ›¿ä»£é»˜è®¤è°ƒåº¦å™¨ï¼ŒHPC ä¼˜å…ˆ     |
| **é›†æˆæ–¹å¼**  | ä¸é»˜è®¤è°ƒåº¦å™¨åä½œ              | ç‹¬ç«‹è°ƒåº¦å™¨                   |
| **æ ¸å¿ƒå…³æ³¨**  | Job çº§é˜Ÿåˆ—ç®¡ç†               | å¤æ‚æ‰¹å¤„ç†å’Œé«˜æ€§èƒ½è®¡ç®—        |

<!--
å»ºè®®é¡µé¢å¸ƒå±€æ”¹è¿›ï¼š
è€ƒè™‘ä½¿ç”¨two-colså¸ƒå±€å±•ç¤ºå¯¹æ¯”
å·¦ä¾§ï¼šKueueæ¶æ„å›¾
å³ä¾§ï¼šVolcanoæ¶æ„å›¾
åº•éƒ¨ï¼šå…³é”®å·®å¼‚æ€»ç»“
-->

---
layout: table
title:  åŠŸèƒ½ç‰¹æ€§å¯¹æ¯”
---

| ç‰¹æ€§          | Kueue                          | Volcano                       |
|:--------------------|:----------------------|:----------------------|
| **é˜Ÿåˆ—ç®¡ç†**  | LocalQueue + ClusterQueue     | Queue                       |
| **ç»„è°ƒåº¦**    | é€šè¿‡ Workload å®ç°            | Gang Scheduling (PodGroup)  |
| **èµ„æºå€Ÿç”¨**  | æ”¯æŒåŠ¨æ€å€Ÿç”¨ä¸å›æ”¶            | é€šè¿‡é˜Ÿåˆ—æƒé‡åˆ†é…            |
| **ä½œä¸šä¾èµ–**  | åŸºç¡€æ”¯æŒ                      | å¼ºå¤§ï¼Œæ”¯æŒå¤æ‚ä¾èµ–          |
| **AI ä¼˜åŒ–**   | é€šç”¨æ”¯æŒ                      | æ·±åº¦é›†æˆ TF, PyTorch ç­‰     |
| **å¤šé›†ç¾¤**    | MultiKueue (å®éªŒæ€§)           | æš‚æ— åŸç”Ÿæ”¯æŒ                |

---
layout: table
title: æ€§èƒ½å¯¹æ¯”
---

| ç‰¹æ€§          | Kueue                          | Volcano                       |
|:--------------------|:----------------------|:----------------------|
| **è°ƒåº¦é€Ÿåº¦** |  ä¾èµ–é»˜è®¤è°ƒåº¦å™¨ï¼Œé€Ÿåº¦ä¸­ç­‰ | è‡ªå®šä¹‰è°ƒåº¦å™¨ï¼Œé€Ÿåº¦æ›´å¿« |
| **èµ„æºåˆ©ç”¨ç‡** | é€šè¿‡å€Ÿç”¨æœºåˆ¶æå‡åˆ©ç”¨ç‡ | é€šè¿‡ç»„è°ƒåº¦å‡å°‘ç¢ç‰‡ |
| **å¤§è§„æ¨¡ä½œä¸š** | é€‚åˆä¸­å°è§„æ¨¡ | æ›´é€‚åˆå¤§è§„æ¨¡ HPC |

---
layout: table
title: é€‚ç”¨åœºæ™¯å¯¹æ¯”
---

| åœºæ™¯                | Kueue æ¨èåº¦          | Volcano æ¨èåº¦        |
|:--------------------|:----------------------|:----------------------|
| **AI/ML è®­ç»ƒ**     | ä¸­ç­‰ (é€šç”¨åœºæ™¯)      | é«˜ (å¤§è§„æ¨¡åˆ†å¸ƒå¼)    |
| **å¤§æ•°æ®æ‰¹å¤„ç†**   | é«˜ (å¤šç§Ÿæˆ·å…¬å¹³æ€§)    | ä¸­ç­‰ (ä¾èµ–å¤æ‚æ€§)    |
| **CI/CD æµæ°´çº¿**   | é«˜ (è½»é‡çº§é›†æˆ)      | ä½ (è¿‡äºé‡å‹)        |
| **HPC ç§‘å­¦è®¡ç®—**   | ä½ (åŠŸèƒ½ä¸è¶³)        | é«˜ (æ€§èƒ½ä¼˜åŒ–)        |

---
layout: table
title: 2025 å‘å±•è·¯çº¿å¯¹æ¯”
---

|å‘å±•æ–¹å‘            |Kueue 2025 è·¯çº¿          |Volcano 2025 è·¯çº¿        |
|:--------------------|:----------------------|:----------------------|
| **å¤šé›†ç¾¤è°ƒåº¦**     | ğŸš€ **MultiKueue å¢å¼º**<br/>ç”¨æˆ·ä½“éªŒä¼˜åŒ–      | ğŸ†• **åŸç”Ÿæ”¯æŒå¼€å‘ä¸­**<br/>è·¨äº‘è·¨é›†ç¾¤è°ƒåº¦    |
| **AI ä¼˜åŒ–**           | ğŸ€ **é€šç”¨ AI æ”¯æŒ**<br/>å¤šæ¡†æ¶é›†æˆ      | ğŸ† **CNAI æ·±åº¦ç‰¹åŒ–**<br/>GPU å…±äº«ã€NUMA æ„ŸçŸ¥    |
| **èµ„æºç®¡ç†**        | ğŸ“Š **å‘³é“åˆ†é…ç­–ç•¥**<br/>æˆæœ¬ vs å€Ÿç”¨ä¼˜åŒ–   | ğŸ“ˆ **å¼¹æ€§åˆ†å±‚é˜Ÿåˆ—**<br/>åŠ¨æ€èµ„æºè¶…å–    |
| **ç”Ÿæ€é›†æˆ**        | ğŸ¤ **å¹¿æ³›é›†æˆ**<br/>Kubeflow, Spark, Ray  | ğŸ’ª **æ·±åº¦é›†æˆ**<br/>Flink, MindSpore åŸç”Ÿæ”¯æŒ |
| **ç»Ÿä¸€è°ƒåº¦**        | â˜®ï¸ **ä¸é»˜è®¤è°ƒåº¦å™¨åä½œ** | ğŸŒ **å®Œå…¨æ›¿ä»£ kube-scheduler**<br/>ç»Ÿä¸€å¾®æœåŠ¡+AI è°ƒåº¦ |

---
layout: two-cols
title: é€‰å‹å»ºè®®
leftTitle: Kueue
rightTitle: Volcano
---

::left::

**é€‚åˆåœºæ™¯**ï¼š
- éœ€è¦ä¸ Kubernetes åŸç”Ÿæ·±åº¦é›†æˆ
- å…³æ³¨å¤šç§Ÿæˆ·èµ„æºå…¬å¹³æ€§
- ä½œä¸šè§„æ¨¡ä¸­ç­‰ï¼Œåé€šç”¨åœºæ™¯

**2025 äº®ç‚¹**ï¼š
- MultiKueue è·¨é›†ç¾¤è°ƒåº¦æˆç†Ÿ
- å‘³é“åˆ†é…ç­–ç•¥æ™ºèƒ½åŒ–
- ç”Ÿæ€é›†æˆæ›´åŠ å¹¿æ³›

::right::

**é€‚åˆåœºæ™¯**ï¼š
- éœ€è¦é«˜æ€§èƒ½è®¡ç®—å’Œ AI ä¼˜åŒ–
- ä½œä¸šä¾èµ–å¤æ‚
- éœ€è¦å®Œå…¨æ§åˆ¶è°ƒåº¦é€»è¾‘

**2025 äº®ç‚¹**ï¼š
- GPU å…±äº«å’Œ NUMA æ„ŸçŸ¥è°ƒåº¦
- ç½‘ç»œæ‹“æ‰‘æ„ŸçŸ¥å’Œå¤šé›†ç¾¤ AI è°ƒåº¦
- å¼¹æ€§åˆ†å±‚é˜Ÿåˆ—å’Œèµ„æºè¶…å–

---
layout: chapter
part: 5
title: AI èµ„æºä¼˜åŒ–ç­–ç•¥
---

---
layout: boxes
title: AI èµ„æºä¼˜åŒ–çš„é‡è¦æ€§
---

**2025å¹´ï¼ŒAIæ¨¡å‹è§„æ¨¡å’Œæ•°æ®é‡æ¿€å¢ï¼Œèµ„æºä¼˜åŒ–æˆä¸ºå…³é”®**

## **æ ¸å¿ƒæŒ‘æˆ˜**

- é«˜è®¡ç®—æˆæœ¬ï¼šå¤§å‹æ¨¡å‹è®­ç»ƒéœ€è¦å¤§é‡GPUèµ„æº
- è´Ÿè½½å˜åŒ–ï¼šæ¨ç†ä»»åŠ¡éœ€ä½å»¶è¿Ÿï¼Œè®­ç»ƒä»»åŠ¡éœ€é«˜ååé‡
- èµ„æºåˆ©ç”¨ç‡ä½ï¼šä¼ ç»Ÿè°ƒåº¦æ–¹å¼å¯¼è‡´èµ„æºæµªè´¹

**é‡åŒ–åˆ†æ**ï¼š
- **GPT-4 è®­ç»ƒæˆæœ¬**ï¼š~$100Mï¼Œ25,000 A100 GPUÃ—3ä¸ªæœˆ
- **æ¨ç†æˆæœ¬**ï¼šæ¯ç™¾ä¸‡ token $0.03-0.12
- **GPU ç©ºé—²ç‡**ï¼šå¹³å‡ 35-45%ï¼Œå³°å€¼å¯è¾¾ 60%
- **èµ„æºç¢ç‰‡åŒ–**ï¼š15-25% GPU å› ç¢ç‰‡æ— æ³•åˆ†é…

## **è§£å†³æ–¹æ¡ˆ**

- æ··éƒ¨è°ƒåº¦ç­–ç•¥
- å¼¹æ€§ä¼¸ç¼©
- èµ„æºè¶…å–

**æŠ€æœ¯çªç ´**ï¼š
- **æ—¶åˆ†å¤ç”¨**ï¼šæ¨ç†ç™½å¤©è¿è¡Œï¼Œè®­ç»ƒå¤œé—´æ‰§è¡Œ
- **ç©ºé—´å¤ç”¨**ï¼šMIG/vGPU æŠ€æœ¯å®ç° GPU å…±äº«
- **æ™ºèƒ½è°ƒåº¦**ï¼šåŸºäºè´Ÿè½½é¢„æµ‹çš„èµ„æºåˆ†é…
- **æˆæœ¬ä¼˜åŒ–**ï¼šSpot å®ä¾‹åˆ©ç”¨ç‡æå‡è‡³ 80%

---
layout: default
title: æ··éƒ¨è°ƒåº¦ç­–ç•¥æ·±åº¦è§£æ
---

## 1. æ··éƒ¨è°ƒåº¦æ¶æ„è®¾è®¡

```mermaid
graph TD
    subgraph "èµ„æºæ± ç®¡ç†"
        A[ç»Ÿä¸€èµ„æºæ± <br/>1000 GPU]
        B[è®­ç»ƒèµ„æºæ± <br/>600 GPU]
        C[æ¨ç†èµ„æºæ± <br/>300 GPU]
        D[å¼¹æ€§èµ„æºæ± <br/>100 GPU]
    end
    
    subgraph "è°ƒåº¦å†³ç­–"
        E[è´Ÿè½½é¢„æµ‹å™¨]
        F[èµ„æºåˆ†é…å™¨]
        G[ä¼˜å…ˆçº§ç®¡ç†]
    end
    
    subgraph "å·¥ä½œè´Ÿè½½"
        H[è®­ç»ƒä»»åŠ¡]
        I[æ¨ç†æœåŠ¡]
        J[å¼€å‘æµ‹è¯•]
    end
    
    A --> E
    E --> F
    F --> B
    F --> C
    F --> D
    G --> F
    H --> B
    I --> C
    J --> D
```

## 2. æŠ€æœ¯å®ç°æ–¹æ¡ˆ

```yaml
# Kueue æ··éƒ¨è°ƒåº¦é…ç½®
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: mixed-workload-queue
spec:
  cohort: gpu-cohort
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["nvidia.com/gpu", "cpu", "memory"]
    flavors:
    # è®­ç»ƒä¸“ç”¨èµ„æº
    - name: training-flavor
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 60
        lendingLimit: 20  # å¯å€Ÿå‡º33%
      nodeLabels:
        workload-type: training
        gpu-memory: 80gb
    
    # æ¨ç†ä¸“ç”¨èµ„æº  
    - name: inference-flavor
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 30
        borrowingLimit: 10  # å¯å€Ÿå…¥33%
      nodeLabels:
        workload-type: inference
        network-latency: low
    
    # å¼¹æ€§å…±äº«èµ„æº
    - name: elastic-flavor
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 10
      nodeLabels:
        workload-type: mixed
```

## 3. æ€§èƒ½æ•°æ®å¯¹æ¯”

| æŒ‡æ ‡ | çº¯è®­ç»ƒé›†ç¾¤ | çº¯æ¨ç†é›†ç¾¤ | æ··éƒ¨é›†ç¾¤ | æå‡ |
|------|-----------|-----------|---------|------|
| **GPU åˆ©ç”¨ç‡** | 65% | 45% | 85% | +30% |
| **æˆæœ¬æ•ˆç‡** | $1.2/TFLOP | $1.8/TFLOP | $0.9/TFLOP | -40% |
| **ä»»åŠ¡ç­‰å¾…æ—¶é—´** | 45min | 15min | 8min | -73% |
| **SLA è¾¾æˆç‡** | 95% | 99% | 97% | - |

## 4. é£é™©æ§åˆ¶æœºåˆ¶

```go
// èµ„æºéš”ç¦»ä¸ QoS ä¿è¯
type MixedScheduler struct {
    // èµ„æºéš”ç¦»çº§åˆ«
    IsolationLevels map[string]IsolationLevel
    // SLA ç›‘æ§å™¨
    SLAMonitor *SLAMonitor
    // èµ„æºå›æ”¶å™¨
    ResourceReclaimer *Reclaimer
}

func (ms *MixedScheduler) Schedule(workload Workload) error {
    // 1. æ£€æŸ¥ SLA è¦æ±‚
    if workload.Type == "inference" && workload.SLA.Latency < 10 {
        // æ¨ç†ä»»åŠ¡éœ€è¦ç‹¬å èµ„æº
        return ms.scheduleExclusive(workload)
    }
    
    // 2. è¯„ä¼°èµ„æºäº‰ç”¨é£é™©
    risk := ms.evaluateContentionRisk(workload)
    if risk > 0.3 {
        // é«˜é£é™©ï¼Œä½¿ç”¨èµ„æºéš”ç¦»
        return ms.scheduleWithIsolation(workload)
    }
    
    // 3. æ­£å¸¸æ··éƒ¨è°ƒåº¦
    return ms.scheduleMixed(workload)
}
```

---
layout: default
title: å¼¹æ€§ä¼¸ç¼©ç­–ç•¥å®æˆ˜
---

## 1. å¤šç»´åº¦å¼¹æ€§ä¼¸ç¼©æ¶æ„

```mermaid
graph LR
    subgraph "æŒ‡æ ‡é‡‡é›†"
        A[GPU ä½¿ç”¨ç‡]
        B[é˜Ÿåˆ—é•¿åº¦]
        C[å“åº”æ—¶é—´]
        D[æˆæœ¬é¢„ç®—]
    end
    
    subgraph "å†³ç­–å¼•æ“"
        E[HPA Controller]
        F[VPA Controller]
        G[Cluster Autoscaler]
        H[KEDA Scaler]
    end
    
    subgraph "æ‰§è¡ŒåŠ¨ä½œ"
        I[å¢åŠ  Pod å‰¯æœ¬]
        J[è°ƒæ•´èµ„æºé…é¢]
        K[æ·»åŠ èŠ‚ç‚¹]
        L[è§¦å‘ä»»åŠ¡]
    end
    
    A --> E
    B --> H
    C --> E
    D --> G
    E --> I
    F --> J
    G --> K
    H --> L
```

## 2. HPA + VPA ç»„åˆé…ç½®

```yaml
# æ¨ç†æœåŠ¡çš„å¼¹æ€§ä¼¸ç¼©é…ç½®
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference-service
  minReplicas: 5
  maxReplicas: 100
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 300
      policies:
      - type: Percent
        value: 10
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 20
        periodSeconds: 15
      selectPolicy: Max
  metrics:
  - type: Resource
    resource:
      name: gpu
      target:
        type: Utilization
        averageUtilization: 75
  - type: Pods
    pods:
      metric:
        name: inference_queue_size
      target:
        type: AverageValue
        averageValue: "30"
---
apiVersion: autoscaling.k8s.io/v1
kind: VerticalPodAutoscaler
metadata:
  name: inference-vpa
spec:
  targetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: llm-inference-service
  updatePolicy:
    updateMode: "Auto"
  resourcePolicy:
    containerPolicies:
    - containerName: inference-container
      minAllowed:
        nvidia.com/gpu: 1
        memory: 16Gi
      maxAllowed:
        nvidia.com/gpu: 4
        memory: 64Gi
```

## 3. KEDA äº‹ä»¶é©±åŠ¨ä¼¸ç¼©

```yaml
# åŸºäº Kafka æ¶ˆæ¯é˜Ÿåˆ—çš„è®­ç»ƒä»»åŠ¡ä¼¸ç¼©
apiVersion: keda.sh/v1alpha1
kind: ScaledJob
metadata:
  name: training-job-scaler
spec:
  jobTargetRef:
    template:
      spec:
        containers:
        - name: trainer
          image: pytorch-training:latest
          resources:
            limits:
              nvidia.com/gpu: 8
  pollingInterval: 30
  maxReplicaCount: 50
  triggers:
  - type: kafka
    metadata:
      bootstrapServers: kafka:9092
      consumerGroup: training-jobs
      topic: model-training-requests
      lagThreshold: "100"
      offsetResetPolicy: latest
```

## 4. å¼¹æ€§ä¼¸ç¼©æ•ˆæœæ•°æ®

| åœºæ™¯ | ä¼ ç»Ÿå›ºå®šèµ„æº | å¼¹æ€§ä¼¸ç¼© | æ”¹å–„ |
|------|-------------|----------|------|
| **æ—¥é—´æ¨ç†å³°å€¼** | 20% è¯·æ±‚è¶…æ—¶ | 0.5% è¯·æ±‚è¶…æ—¶ | 97.5% â†“ |
| **å¤œé—´è®­ç»ƒ** | 40% GPU ç©ºé—² | 5% GPU ç©ºé—² | 87.5% â†“ |
| **çªå‘æµé‡** | ç³»ç»Ÿå´©æºƒ | è‡ªåŠ¨æ‰©å®¹åº”å¯¹ | 100% å¯ç”¨æ€§ |
| **æˆæœ¬æ§åˆ¶** | $50K/æœˆ | $32K/æœˆ | 36% â†“ |

---
layout: default
title: èµ„æºè¶…å–æŠ€æœ¯è¯¦è§£
---

## 1. èµ„æºè¶…å–åŸç†

```go
// èµ„æºè¶…å–æ ¸å¿ƒç®—æ³•
type OversubscriptionManager struct {
    // ç‰©ç†èµ„æº
    PhysicalResources Resources
    // å·²åˆ†é…è™šæ‹Ÿèµ„æº
    VirtualAllocated Resources
    // è¶…å–æ¯”ä¾‹
    OversubscriptionRatio map[string]float64
    // èµ„æºä½¿ç”¨å†å²
    UsageHistory *RingBuffer
}

func (om *OversubscriptionManager) CanAllocate(request Resources) bool {
    // 1. è®¡ç®—å®é™…ä½¿ç”¨ç‡
    actualUsage := om.calculateActualUsage()
    
    // 2. è¯„ä¼°è¶…å–é£é™©
    for resource, amount := range request {
        physicalCapacity := om.PhysicalResources[resource]
        currentVirtual := om.VirtualAllocated[resource]
        oversubRatio := om.OversubscriptionRatio[resource]
        
        // æ£€æŸ¥æ˜¯å¦è¶…è¿‡å®‰å…¨é˜ˆå€¼
        if (currentVirtual + amount) > (physicalCapacity * oversubRatio) {
            return false
        }
        
        // åŸºäºå†å²ä½¿ç”¨é¢„æµ‹é£é™©
        predictedPeak := om.predictPeakUsage(resource)
        if predictedPeak + amount > physicalCapacity * 0.95 {
            return false
        }
    }
    
    return true
}

func (om *OversubscriptionManager) predictPeakUsage(resource string) float64 {
    // ä½¿ç”¨ EWMA ç®—æ³•é¢„æµ‹å³°å€¼
    history := om.UsageHistory.GetResourceHistory(resource)
    alpha := 0.3 // å¹³æ»‘å› å­
    
    ewma := history[0]
    for i := 1; i < len(history); i++ {
        ewma = alpha*history[i] + (1-alpha)*ewma
    }
    
    // åŠ ä¸Šå®‰å…¨è¾¹é™…
    return ewma * 1.2
}
```

## 2. QoS åˆ†çº§ç®¡ç†

```yaml
# èµ„æºè¶…å– QoS é…ç½®
apiVersion: v1
kind: ResourceQuota
metadata:
  name: guaranteed-quota
spec:
  hard:
    requests.nvidia.com/gpu: "100"
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values: ["guaranteed"]
---
apiVersion: v1
kind: ResourceQuota
metadata:
  name: burstable-quota
spec:
  hard:
    requests.nvidia.com/gpu: "150"  # 150% è¶…å–
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values: ["burstable"]
---
apiVersion: v1
kind: ResourceQuota  
metadata:
  name: besteffort-quota
spec:
  hard:
    requests.nvidia.com/gpu: "200"  # 200% è¶…å–
  scopeSelector:
    matchExpressions:
    - scopeName: PriorityClass
      operator: In
      values: ["besteffort"]
```

## 3. æ™ºèƒ½èµ„æºå›æ”¶

```python
# åŸºäºæœºå™¨å­¦ä¹ çš„èµ„æºå›æ”¶å†³ç­–
import numpy as np
from sklearn.ensemble import RandomForestRegressor

class IntelligentReclaimer:
    def __init__(self):
        self.model = RandomForestRegressor(n_estimators=100)
        self.feature_names = [
            'current_usage', 'time_of_day', 'day_of_week',
            'workload_type', 'priority', 'duration'
        ]
    
    def train(self, historical_data):
        """è®­ç»ƒèµ„æºä½¿ç”¨é¢„æµ‹æ¨¡å‹"""
        X = historical_data[self.feature_names]
        y = historical_data['peak_usage']
        self.model.fit(X, y)
    
    def should_reclaim(self, workload):
        """å†³å®šæ˜¯å¦å›æ”¶èµ„æº"""
        features = self.extract_features(workload)
        predicted_usage = self.model.predict([features])[0]
        
        # å¦‚æœé¢„æµ‹ä½¿ç”¨ç‡ä½äº 30%ï¼Œåˆ™å›æ”¶
        if predicted_usage < 0.3:
            return True, 1.0 - predicted_usage
        
        return False, 0.0
    
    def calculate_reclaim_amount(self, workload, reclaim_ratio):
        """è®¡ç®—å›æ”¶èµ„æºé‡"""
        allocated = workload.allocated_resources
        return {
            'gpu': int(allocated['gpu'] * reclaim_ratio * 0.8),  # ä¿å®ˆå›æ”¶
            'memory': int(allocated['memory'] * reclaim_ratio * 0.7)
        }
```

## 4. ç”Ÿäº§ç¯å¢ƒæ•ˆæœ

| æŒ‡æ ‡ | æ— è¶…å– | ä¿å®ˆè¶…å–(1.3x) | æ¿€è¿›è¶…å–(2.0x) |
|------|--------|---------------|---------------|
| **èµ„æºåˆ©ç”¨ç‡** | 65% | 84% | 92% |
| **ä»»åŠ¡å¤±è´¥ç‡** | 0.1% | 0.3% | 2.1% |
| **æˆæœ¬èŠ‚çœ** | åŸºå‡† | 23% | 45% |
| **SLA è¿çº¦** | 0.05% | 0.1% | 0.8% |

---
layout: chapter
part: 6
title: ååŒè°ƒåº¦ï¼šè®­ç»ƒä¸æ¨ç†å…±å­˜
---

---
layout: default
title: è®­ç»ƒä¸æ¨ç†ååŒè°ƒåº¦çš„æŒ‘æˆ˜
---

**åœ¨åŒä¸€é›†ç¾¤ä¸­è¿è¡Œè®­ç»ƒå’Œæ¨ç†ä»»åŠ¡çš„æŠ€æœ¯éš¾é¢˜**

- **èµ„æºéœ€æ±‚å·®å¼‚**:
  - æ¨ç†ä»»åŠ¡ï¼šä½å»¶è¿Ÿã€ç¨³å®šèµ„æºéœ€æ±‚
  - è®­ç»ƒä»»åŠ¡ï¼šé«˜ååé‡ã€å¼¹æ€§èµ„æºéœ€æ±‚

- **æŠ€æœ¯æŒ‘æˆ˜**:
  - èµ„æºç«äº‰å¯èƒ½å½±å“æ¨ç†æ€§èƒ½
  - å¦‚ä½•ä¿è¯æ¨ç†ä»»åŠ¡çš„SLA
  - åŠ¨æ€è´Ÿè½½ä¸‹çš„èµ„æºåˆ†é…ç­–ç•¥

<!--
å»ºè®®ï¼š
å¢åŠ ååŒè°ƒåº¦æ¶æ„å›¾ï¼š
- è®­ç»ƒ/æ¨ç†èµ„æºæ± åˆ’åˆ†
- åŠ¨æ€èµ„æºæµè½¬ç¤ºæ„å›¾
- ç›‘æ§æŒ‡æ ‡å±•ç¤º
-->

---
layout: default
title: DeepBoot ååŒè°ƒåº¦ç³»ç»Ÿ
---

**ä¸šç•Œé¢†å…ˆçš„è®­ç»ƒæ¨ç†ååŒè°ƒåº¦è§£å†³æ–¹æ¡ˆ**

- **è‡ªé€‚åº”ä»»åŠ¡ä¼¸ç¼© (ATS)**:
  - åŠ¨æ€åˆ†é…è®­ç»ƒå’Œæ¨ç†é›†ç¾¤çš„GPU
  - åŸºäºè´Ÿè½½æƒ…å†µå®æ—¶è°ƒæ•´èµ„æºåˆ†é…

- **è‡ªåŠ¨å¿«é€Ÿå¼¹æ€§è®­ç»ƒ (AFE)**:
  - åŸºäºPolluxæŠ€æœ¯ï¼Œå‡å°‘GPUå›æ”¶æ—¶çš„é‡å¯å¼€é”€
  - æ”¯æŒæ£€æŸ¥ç‚¹å’Œå¿«é€Ÿæ¢å¤

```mermaid
graph TD
    A[DeepBootç³»ç»Ÿ] --> B[ATSè‡ªé€‚åº”ä¼¸ç¼©]
    A --> C[AFEå¼¹æ€§è®­ç»ƒ]
    B --> D[æ¨ç†é›†ç¾¤]
    B --> E[è®­ç»ƒé›†ç¾¤]
```

---
layout: default
title: ååŒè°ƒåº¦é…ç½®ç¤ºä¾‹
---

**ä½¿ç”¨ VolcanoJob å®ç°è®­ç»ƒæ¨ç†æ··åˆè°ƒåº¦**

```yaml
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: ai-mixed-job
spec:
  minAvailable: 3
  schedulerName: volcano
  priorityClassName: high-priority
  tasks:
    - replicas: 2
      name: training-task
      template:
        spec:
          containers:
            - name: trainer
              image: tensorflow:latest
              resources:
                requests:
                  nvidia.com/gpu: "1"
    - replicas: 4
      name: inference-task
      template:
        spec:
          containers:
            - name: inferencer
              image: tensorflow-serving:latest
              resources:
                requests:
                  nvidia.com/gpu: "0.5"
```

---
layout: default
title: å¼€æºç”Ÿæ€ç³»ç»Ÿ
---

**æ”¯æŒAIèµ„æºä¼˜åŒ–çš„æ ¸å¿ƒå¼€æºé¡¹ç›®**

| é¡¹ç›® | åŠŸèƒ½ | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| **KEDA** | äº‹ä»¶é©±åŠ¨è‡ªåŠ¨ä¼¸ç¼© | æ¨ç†æœåŠ¡å¼¹æ€§ä¼¸ç¼© |
| **Ray** | åˆ†å¸ƒå¼è®¡ç®—æ¡†æ¶ | è®­ç»ƒæ¨ç†ç»Ÿä¸€è°ƒåº¦ |
| **Pollux** | ååŒè‡ªé€‚åº”è°ƒåº¦ | æ·±åº¦å­¦ä¹ ä»»åŠ¡ä¼˜åŒ– |
| **Prometheus** | ç›‘æ§å‘Šè­¦ | èµ„æºä½¿ç”¨ç›‘æ§ |

---
layout: default
title: æœ€ä½³å®è·µä¸é…ç½®
---

**ç”Ÿäº§ç¯å¢ƒä¸­çš„å…³é”®é…ç½®ç­–ç•¥**

- **HPA é…ç½®ç¤ºä¾‹**:
```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: inference-hpa
spec:
  scaleTargetRef:
    name: inference-service
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          averageUtilization: 70
```

- **ä¼˜å…ˆçº§ç®¡ç†**: ä½¿ç”¨PriorityClassç¡®ä¿æ¨ç†ä»»åŠ¡ä¼˜å…ˆçº§
- **èµ„æºé…é¢**: åˆç†è®¾ç½®CPUå’ŒGPUè¯·æ±‚/é™åˆ¶

<!--
å»ºè®®ï¼šæ·»åŠ Kueue waitForPodsReadyé…ç½®
```yaml
waitForPodsReady:
  enable: true
  timeout: 10m
  recoveryTimeout: 3m
  blockAdmission: true
  requeuingStrategy:
    timestamp: Eviction
    backoffLimitCount: 5
    backoffBaseSeconds: 60
```
-->

---
layout: chapter
part: 7
title: å®æˆ˜æ¼”ç»ƒ
---

---
layout: default
title: ç¯å¢ƒå‡†å¤‡
---

- **é›†ç¾¤**: Kubernetes v1.25+
- **å·¥å…·**: kubectl, kueuectl, volcano cli
- **å®‰è£… Kueue**:
  ```bash
  kubectl apply -f https://github.com/kubernetes-sigs/kueue/releases/download/v0.5.0/manifests.yaml
  ```
- **å®‰è£… Volcano**:
  ```bash
  kubectl apply -f https://github.com/volcano-sh/volcano/releases/download/v1.8.0/volcano.yaml
  ```

<!--
å»ºè®®ï¼š
- æ›´æ–°å®‰è£…å‘½ä»¤åˆ°æœ€æ–°ç‰ˆæœ¬
- å¢åŠ Helmå®‰è£…æ–¹å¼
- æ·»åŠ ç”Ÿäº§ç¯å¢ƒé…ç½®æœ€ä½³å®è·µ
- å¢åŠ æ•…éšœæ’æŸ¥å†³ç­–æ ‘

æ¯ä¸ªå®æˆ˜é¡µé¢éœ€è¦ï¼š
- å‰ç½®æ¡ä»¶æ£€æŸ¥æ¸…å•
- åˆ†æ­¥éª¤æˆªå›¾
- å¸¸è§é”™è¯¯å’Œè§£å†³æ–¹æ¡ˆ
- æ€§èƒ½è°ƒä¼˜å»ºè®®
-->

---
layout: default
title: Kueue å®æˆ˜ï¼šé…ç½® ClusterQueue
---

```yaml
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: ai-training-queue
spec:
  namespaceSelector: {}
  resourceGroups:
  - coveredResources: ["cpu", "memory", "nvidia.com/gpu"]
    flavors:
    - name: gpu-flavor
      resources:
      - name: cpu
        nominalQuota: 16
      - name: memory
        nominalQuota: 32Gi
      - name: nvidia.com/gpu
        nominalQuota: 4
        lendingLimit: 2
```

---
layout: default
title: Kueue å®æˆ˜ï¼šæäº¤ä½œä¸š
---

```yaml
apiVersion: batch/v1
kind: Job
metadata:
  name: ai-training-job
  annotations:
    kueue.x-k8s.io/queue-name: ai-team-queue
spec:
  template:
    spec:
      containers:
      - name: trainer
        image: tensorflow/tensorflow:2.12.0-gpu
        resources:
          limits:
            nvidia.com/gpu: 2
      restartPolicy: OnFailure
```

---
layout: default
title: Kueue å®æˆ˜ï¼šç›‘æ§è°ƒåº¦
---

- **æŸ¥çœ‹é˜Ÿåˆ—çŠ¶æ€**:
  ```bash
  kueuectl list localqueue -n ai-team
  ```
- **æŸ¥çœ‹ä½œä¸šçŠ¶æ€**:
  ```bash
  kubectl get workload -n ai-team
  ```
- **èµ„æºå€Ÿç”¨æƒ…å†µ**:
  ```bash
  kubectl describe clusterqueue ai-training-queue
  ```

---
layout: default
title: Kueue å®æˆ˜ï¼šæ•…éšœæ’æŸ¥
---

- **ä½œä¸šå¡åœ¨æ’é˜Ÿ**:
  - æ£€æŸ¥ ClusterQueue èµ„æºæ˜¯å¦è€—å°½
  - æŸ¥çœ‹æ˜¯å¦æœ‰æ›´é«˜ä¼˜å…ˆçº§ä½œä¸šæŠ¢å 
- **èµ„æºå€Ÿç”¨å¤±è´¥**:
  - æ£€æŸ¥ lendingLimit æ˜¯å¦è¿‡ä½
  - ç¡®è®¤æ˜¯å¦æœ‰å…¶ä»–é˜Ÿåˆ—å¯ç”¨èµ„æº

---
layout: default
title: Volcano å®æˆ˜ï¼šé…ç½® Queue
---

```yaml
apiVersion: scheduling.volcano.sh/v1beta1
kind: Queue
metadata:
  name: ai-training-queue
spec:
  weight: 10
  capability:
    cpu: 16
    memory: 32Gi
    nvidia.com/gpu: 4
```

---
layout: default
title: Volcano å®æˆ˜ï¼šæäº¤ VolcanoJob
---

```yaml
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: distributed-training
spec:
  minAvailable: 4
  tasks:
  - replicas: 4
    name: trainer
    template:
      spec:
        containers:
        - name: tf-trainer
          image: tensorflow/tensorflow:2.12.0-gpu
          resources:
            limits:
              nvidia.com/gpu: 1
        restartPolicy: OnFailure
```

---
layout: default
title: Volcano å®æˆ˜ï¼šç›‘æ§è°ƒåº¦
---

- **æŸ¥çœ‹é˜Ÿåˆ—**:
  ```bash
  kubectl get queue -n volcano-system
  ```
- **æŸ¥çœ‹ä½œä¸š**:
  ```bash
  kubectl get job -n ai-team
  ```
- **æŸ¥çœ‹ PodGroup**:
  ```bash
  kubectl get podgroup -n ai-team
  ```

---
layout: default
title: Volcano å®æˆ˜ï¼šæ•…éšœæ’æŸ¥
---

- **ä½œä¸šæœªè°ƒåº¦**:
  - æ£€æŸ¥ Queue èµ„æºé…é¢
  - ç¡®è®¤ minAvailable æ˜¯å¦æ»¡è¶³
- **PodGroup å¡ä½**:
  - æŸ¥çœ‹æ˜¯å¦æœ‰èŠ‚ç‚¹èµ„æºç¢ç‰‡
  - æ£€æŸ¥æ˜¯å¦æœ‰æ›´é«˜ä¼˜å…ˆçº§ä½œä¸š

---
layout: boxes
title: æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹
---

## **Kueue**

- è°ƒæ•´ lendingLimit æé«˜èµ„æºåˆ©ç”¨ç‡
- ä½¿ç”¨ WorkloadPriorityClass ä¼˜åŒ–å…³é”®ä½œä¸š

**ä¼˜åŒ–ç»†èŠ‚**ï¼š
- **åŠ¨æ€å€Ÿç”¨ç­–ç•¥**ï¼šåŸºäºæ—¶é—´çª—å£çš„è‡ªé€‚åº” lendingLimit
- **ä¼˜å…ˆçº§ç»†åˆ†**ï¼š5 çº§ä¼˜å…ˆçº§ä½“ç³»ï¼ŒæŠ¢å å»¶è¿Ÿ < 5s
- **é˜Ÿåˆ—æ‹“æ‰‘ä¼˜åŒ–**ï¼šå±‚çº§é˜Ÿåˆ—å‡å°‘è°ƒåº¦å†³ç­–æ—¶é—´ 40%

## **Volcano**

- è°ƒæ•´ Queue æƒé‡å¹³è¡¡å¤šå›¢é˜Ÿéœ€æ±‚
- ä½¿ç”¨ Gang Scheduling å‡å°‘èµ„æºæ­»é”

**ä¼˜åŒ–æŠ€å·§**ï¼š
- **æ’ä»¶ç»„åˆ**ï¼šGang + DRF + Binpack æœ€ä¼˜é…ç½®
- **è°ƒåº¦å‘¨æœŸè°ƒä¼˜**ï¼šå¤§ä½œä¸š 1sï¼Œå°ä½œä¸š 100ms
- **ç¼“å­˜é¢„çƒ­**ï¼šå¯åŠ¨æ—¶åŠ è½½å†å²è°ƒåº¦æ•°æ®

---
layout: default
title: æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹è¯¦è§£ï¼šå­—èŠ‚è·³åŠ¨ AI å¹³å°
---

## 1. èƒŒæ™¯ä¸æŒ‘æˆ˜

**é›†ç¾¤è§„æ¨¡**ï¼š
- 10,000+ GPU (V100/A100/H100 æ··åˆ)
- 50,000+ CPU èŠ‚ç‚¹
- æ—¥å‡ 100K+ ä½œä¸šæäº¤
- å³°å€¼ 5K å¹¶å‘ä½œä¸š

**æ ¸å¿ƒæŒ‘æˆ˜**ï¼š
1. **èµ„æºåˆ©ç”¨ç‡ä½**ï¼šGPU å¹³å‡åˆ©ç”¨ç‡ä»… 55%
2. **è°ƒåº¦å»¶è¿Ÿé«˜**ï¼šP99 è°ƒåº¦å»¶è¿Ÿè¾¾ 5 åˆ†é’Ÿ
3. **æˆæœ¬å‹åŠ›å¤§**ï¼šå¹´åº¦ GPU æˆæœ¬è¶… $200M
4. **å¤šæ¡†æ¶æ··éƒ¨**ï¼šTensorFlowã€PyTorchã€PaddlePaddle å…±å­˜

## 2. ä¼˜åŒ–æ–¹æ¡ˆå®æ–½

### 2.1 Kueue é…ç½®ä¼˜åŒ–

```yaml
# å¤šçº§é˜Ÿåˆ—é…ç½®
apiVersion: kueue.x-k8s.io/v1beta1
kind: ClusterQueue
metadata:
  name: bytedance-ai-root
spec:
  cohort: bytedance-cohort
  queueingStrategy: BestEffortFIFO  # æ”¹ä¸º BestEffortFIFO
  namespaceSelector:
    matchLabels:
      platform: "ai"
  resourceGroups:
  - coveredResources: ["nvidia.com/gpu", "cpu", "memory"]
    flavors:
    - name: training-a100
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 2000
        lendingLimit: 600   # 30% å¯å€Ÿå‡º
        borrowingLimit: 800 # 40% å¯å€Ÿå…¥
    - name: inference-v100  
      resources:
      - name: nvidia.com/gpu
        nominalQuota: 1000
        lendingLimit: 300
  fairSharing:
    enable: true
    weight: 1
  preemption:
    reclaimWithinCohort: Any
    borrowWithinCohort:
      policy: LowerOrNewerEqualPriority
      maxPriorityThreshold: 100
```

### 2.2 Volcano è°ƒåº¦ç­–ç•¥ä¼˜åŒ–

```go
// è‡ªå®šä¹‰è°ƒåº¦æ’ä»¶ï¼šGPU äº²å’Œæ€§ä¼˜åŒ–
package custom

import (
    "volcano.sh/volcano/pkg/scheduler/api"
    "volcano.sh/volcano/pkg/scheduler/framework"
)

type GPUAffinityPlugin struct {
    // GPU æ‹“æ‰‘ä¿¡æ¯ç¼“å­˜
    topologyCache map[string]*GPUTopology
}

func (gap *GPUAffinityPlugin) OnSessionOpen(ssn *framework.Session) {
    // æ³¨å†ŒèŠ‚ç‚¹æ‰“åˆ†å‡½æ•°
    ssn.AddNodeOrderFn(gap.Name(), func(task *api.TaskInfo, node *api.NodeInfo) (float64, error) {
        score := 0.0
        
        // 1. æ£€æŸ¥ NVLink è¿æ¥æ€§
        if gap.hasNVLinkConnectivity(node, task.Requests.ScalarResources["nvidia.com/gpu"]) {
            score += 50.0
        }
        
        // 2. æ•°æ®æœ¬åœ°æ€§è¯„åˆ†
        if gap.hasLocalData(task, node) {
            score += 30.0
        }
        
        // 3. ç½‘ç»œå¸¦å®½è¯„åˆ†
        bandwidth := gap.getNetworkBandwidth(node)
        score += float64(bandwidth) / 100.0 * 20.0
        
        return score, nil
    })
}
```

## 3. ä¼˜åŒ–æ•ˆæœæ•°æ®

### 3.1 èµ„æºåˆ©ç”¨ç‡æå‡

```mermaid
graph LR
    subgraph "ä¼˜åŒ–å‰"
        A1[GPU: 55%<br/>CPU: 45%<br/>Memory: 60%]
    end
    
    subgraph "ä¼˜åŒ–å"
        B1[GPU: 87%<br/>CPU: 78%<br/>Memory: 85%]
    end
    
    A1 -->|+58%| B1
```

### 3.2 å…³é”®æŒ‡æ ‡æ”¹å–„

| æŒ‡æ ‡ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æ”¹å–„ |
|------|--------|--------|------|
| **GPU åˆ©ç”¨ç‡** | 55% | 87% | +58% |
| **è°ƒåº¦å»¶è¿Ÿ P50** | 30s | 5s | -83% |
| **è°ƒåº¦å»¶è¿Ÿ P99** | 5min | 30s | -90% |
| **ä½œä¸šå®Œæˆæ—¶é—´** | 4.5h | 2.8h | -38% |
| **èµ„æºç¢ç‰‡ç‡** | 22% | 7% | -68% |
| **å¹´åº¦æˆæœ¬** | $200M | $145M | -27.5% |

## 4. ç›‘æ§ä¸å‘Šè­¦é…ç½®

```yaml
# Prometheus ç›‘æ§è§„åˆ™
groups:
- name: scheduler_performance
  interval: 30s
  rules:
  - alert: HighSchedulingLatency
    expr: histogram_quantile(0.99, kueue_admission_latency_seconds) > 10
    for: 5m
    annotations:
      summary: "è°ƒåº¦å»¶è¿Ÿè¿‡é«˜"
      
  - alert: LowGPUUtilization
    expr: avg(nvidia_gpu_duty_cycle) < 0.7
    for: 10m
    annotations:
      summary: "GPU åˆ©ç”¨ç‡ä½äº 70%"
      
  - alert: ResourceFragmentation
    expr: (1 - sum(allocated_resources) / sum(total_resources)) > 0.15
    for: 15m
    annotations:
      summary: "èµ„æºç¢ç‰‡ç‡è¶…è¿‡ 15%"
```

---
layout: default
title: æ€§èƒ½ä¼˜åŒ–æ¡ˆä¾‹è¯¦è§£ï¼šé˜¿é‡Œäº‘ PAI å¹³å°
---

## 1. åœºæ™¯æè¿°

**ä¸šåŠ¡ç‰¹ç‚¹**ï¼š
- æ··åˆå·¥ä½œè´Ÿè½½ï¼š70% è®­ç»ƒï¼Œ30% æ¨ç†
- å¤šç§Ÿæˆ·ï¼š500+ å†…éƒ¨å›¢é˜Ÿå…±äº«
- æˆæœ¬æ•æ„Ÿï¼šéœ€è¦æè‡´çš„æˆæœ¬ä¼˜åŒ–

## 2. Volcano æ·±åº¦ä¼˜åŒ–

### 2.1 è‡ªå®šä¹‰ Action å¼€å‘

```go
// æ½®æ±è°ƒåº¦ Action
type TidalAction struct {
    ssn *framework.Session
}

func (ta *TidalAction) Execute(ssn *framework.Session) {
    // è·å–å½“å‰æ—¶é—´æ®µ
    hour := time.Now().Hour()
    isBusinessHour := hour >= 9 && hour <= 18
    
    // åŠ¨æ€è°ƒæ•´èµ„æºåˆ†é…
    for _, queue := range ssn.Queues {
        if isBusinessHour {
            // ç™½å¤©ï¼šæ¨ç†ä¼˜å…ˆ
            if queue.Name == "inference-queue" {
                queue.Spec.Weight = 70
            } else if queue.Name == "training-queue" {
                queue.Spec.Weight = 30
            }
        } else {
            // å¤œé—´ï¼šè®­ç»ƒä¼˜å…ˆ
            if queue.Name == "training-queue" {
                queue.Spec.Weight = 80
            } else if queue.Name == "inference-queue" {
                queue.Spec.Weight = 20
            }
        }
    }
}
```

### 2.2 æ€§èƒ½è°ƒä¼˜å‚æ•°

```yaml
# Volcano è°ƒåº¦å™¨é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: volcano-scheduler-configmap
data:
  volcano-scheduler.conf: |
    actions: "enqueue, allocate, backfill, reclaim, preempt"
    tiers:
    - plugins:
      - name: priority
      - name: gang
        arguments:
          "preempt-level": "job"
      - name: conformance
    - plugins:
      - name: drf
        arguments:
          "enable-preempt": "true"
      - name: predicates
      - name: nodeorder
        arguments:
          "weight.gpu": "10"
          "weight.cpu": "1"
          "weight.memory": "1"
```

## 3. æˆæœ¬ä¼˜åŒ–æ•ˆæœ

### 3.1 Spot å®ä¾‹åˆ©ç”¨

```python
# æˆæœ¬ä¼˜åŒ–ç­–ç•¥
class SpotOptimizer:
    def __init__(self):
        self.spot_price_history = {}
        self.interruption_predictor = InterruptionModel()
    
    def optimize_placement(self, job):
        """ä¼˜åŒ–ä½œä¸šæ”¾ç½®ç­–ç•¥"""
        if job.fault_tolerant and job.priority < 100:
            # ä½ä¼˜å…ˆçº§å®¹é”™ä»»åŠ¡ä½¿ç”¨ Spot
            spot_nodes = self.get_available_spot_nodes()
            
            # é¢„æµ‹ä¸­æ–­æ¦‚ç‡
            for node in spot_nodes:
                risk = self.interruption_predictor.predict(node)
                if risk < 0.1:  # ä¸­æ–­é£é™© < 10%
                    return self.place_on_spot(job, node)
        
        # å¦åˆ™ä½¿ç”¨æŒ‰éœ€å®ä¾‹
        return self.place_on_demand(job)
```

### 3.2 æˆæœ¬èŠ‚çœåˆ†æ

| å®ä¾‹ç±»å‹ | ä¼˜åŒ–å‰å æ¯” | ä¼˜åŒ–åå æ¯” | å•ä»·($/h) | æœˆæˆæœ¬èŠ‚çœ |
|----------|-----------|-----------|-----------|------------|
| **æŒ‰éœ€ A100** | 80% | 45% | 3.0 | $756K |
| **Spot A100** | 10% | 35% | 0.9 | - |
| **æŒ‰éœ€ V100** | 10% | 5% | 2.1 | $88K |
| **Spot V100** | 0% | 15% | 0.6 | - |
| **æ€»è®¡** | - | - | - | **$844K/æœˆ** |

## 4. ç»éªŒæ€»ç»“

**å…³é”®æˆåŠŸå› ç´ **ï¼š
1. **ç²¾ç»†åŒ–èµ„æºç”»åƒ**ï¼šå»ºç«‹ä½œä¸šç‰¹å¾æ•°æ®åº“
2. **æ™ºèƒ½è°ƒåº¦å†³ç­–**ï¼šåŸºäº ML çš„è´Ÿè½½é¢„æµ‹
3. **å¼¹æ€§èµ„æºæ± **ï¼šSpot + Reserved + On-demand æ··åˆ
4. **æŒç»­ä¼˜åŒ–**ï¼šA/B æµ‹è¯•ä¸åŒè°ƒåº¦ç­–ç•¥

**è¸©å‘ç»éªŒ**ï¼š
1. **Spot ä¸­æ–­å¤„ç†**ï¼šæ£€æŸ¥ç‚¹ä¿å­˜é¢‘ç‡éœ€è¦å¹³è¡¡
2. **ä¼˜å…ˆçº§å€’æŒ‚**ï¼šéœ€è¦é˜²æ­¢ä½ä¼˜å…ˆçº§ä»»åŠ¡é¥¿æ­»
3. **ç›‘æ§è¦†ç›–**ï¼šç»†ç²’åº¦ç›‘æ§æ˜¯ä¼˜åŒ–çš„åŸºç¡€

---
layout: chapter
part: 8
title:  æ€»ç»“ä¸å±•æœ›
---

---
layout: default
title: æ ¸å¿ƒæ€»ç»“
---

**è°ƒåº¦å™¨å¯¹æ¯”**
- **Kueue**: Kubernetes åŸç”Ÿå¢å¼ºï¼Œé€‚åˆå¤šç§Ÿæˆ·å’Œé€šç”¨æ‰¹å¤„ç†
- **Volcano**: é«˜æ€§èƒ½è®¡ç®—ä¼˜åŒ–ï¼Œé€‚åˆ AI/HPC å’Œå¤æ‚ä¾èµ–
- **é€‰å‹å…³é”®**: æ ¹æ®å·¥ä½œè´Ÿè½½ç‰¹æ€§å’Œé›†æˆéœ€æ±‚é€‰æ‹©

**AI èµ„æºä¼˜åŒ–æˆæœ**
- **æ··éƒ¨è°ƒåº¦**: å®ç°è®­ç»ƒæ¨ç†ç»Ÿä¸€è°ƒåº¦ï¼Œèµ„æºåˆ©ç”¨ç‡æå‡ 30-50%
- **å¼¹æ€§ä¼¸ç¼©**: åŠ¨æ€å“åº”è´Ÿè½½å˜åŒ–ï¼Œé™ä½æˆæœ¬ 20-40%
- **èµ„æºè¶…å–**: æ™ºèƒ½åˆ†é…é—²ç½®èµ„æºï¼Œæ•´ä½“æ•ˆç‡æå‡ 25-35%

---
layout: default
title: AI èµ„æºä¼˜åŒ–çš„æ ¸å¿ƒä»·å€¼
---

**æŠ€æœ¯çªç ´**
- **ååŒè°ƒåº¦æ¶æ„**: DeepBoot ç­‰ç³»ç»Ÿå®ç°è®­ç»ƒæ¨ç†æ— ç¼åˆ‡æ¢
- **å¤šç»´åº¦å¼¹æ€§**: HPA/VPA/KEDA æ„å»ºå…¨æ–¹ä½è‡ªåŠ¨ä¼¸ç¼©ä½“ç³»
- **æ™ºèƒ½èµ„æºç®¡ç†**: åŸºäº QoS çš„åŠ¨æ€è¶…å–ä¸ä¼˜å…ˆçº§è°ƒåº¦

**ä¸šåŠ¡ä»·å€¼**
- **æˆæœ¬ä¼˜åŒ–**: GPU åˆ©ç”¨ç‡ä» 40% æå‡è‡³ 75%+
- **æ€§èƒ½ä¿éšœ**: æ¨ç†å»¶è¿Ÿ SLA è¾¾æˆç‡ > 99.5%
- **è¿ç»´ç®€åŒ–**: è‡ªåŠ¨åŒ–èµ„æºåˆ†é…ï¼Œäººå·¥å¹²é¢„å‡å°‘ 80%

---
layout: default
title: å¼€æºç”Ÿæ€ç³»ç»Ÿçš„æˆç†Ÿåº¦
---

**ç¬¬ä¸€æ¢¯é˜Ÿé¡¹ç›®**ï¼ˆç”Ÿäº§å°±ç»ªï¼‰
- **KEDA**: äº‹ä»¶é©±åŠ¨å¼¹æ€§ä¼¸ç¼©çš„äº‹å®æ ‡å‡†
- **Prometheus**: ç›‘æ§å‘Šè­¦ç”Ÿæ€å®Œå–„ï¼Œé›†æˆåº¦é«˜
- **Volcano**: AI/HPC è°ƒåº¦é¢†åŸŸçš„é¢†å¯¼è€…

**æ–°å…´é¡¹ç›®**ï¼ˆå¿«é€Ÿå‘å±•ï¼‰
- **Ray**: åˆ†å¸ƒå¼ AI è®¡ç®—å¹³å°ï¼Œç¤¾åŒºæ´»è·ƒ
- **Pollux**: ååŒè°ƒåº¦ç®—æ³•åˆ›æ–°ï¼Œå­¦æœ¯ç•Œè®¤å¯
- **MultiKueue**: è·¨é›†ç¾¤è°ƒåº¦çš„æœªæ¥è¶‹åŠ¿

---
layout: default
title: æœªæ¥å±•æœ›ï¼šæŠ€æœ¯æ¼”è¿›
---

**çŸ­æœŸè¶‹åŠ¿ (2025-2026)**
- **Serverless AI**: æ— æœåŠ¡å™¨æ¶æ„ç®€åŒ– AI åº”ç”¨éƒ¨ç½²
- **GPU è™šæ‹ŸåŒ–**: MIGã€vGPU æŠ€æœ¯æ™®åŠï¼Œèµ„æºç²’åº¦æ›´ç»†
- **è¾¹ç¼˜ AI è°ƒåº¦**: äº‘è¾¹ååŒï¼Œæ”¯æŒç«¯åˆ°ç«¯ AI å·¥ä½œæµ

**ä¸­æœŸè¶‹åŠ¿ (2027-2028)**
- **AI é©±åŠ¨è°ƒåº¦**: ä½¿ç”¨ AI é¢„æµ‹è´Ÿè½½ï¼Œä¼˜åŒ–èµ„æºåˆ†é…ç­–ç•¥
- **é‡å­è®¡ç®—é›†æˆ**: æ”¯æŒé‡å­-ç»å…¸æ··åˆè®¡ç®—è°ƒåº¦
- **ç¢³ä¸­å’Œä¼˜åŒ–**: åŸºäºèƒ½è€—å’Œç¢³æ’æ”¾çš„ç»¿è‰²è°ƒåº¦ç®—æ³•

---
layout: image
image: public/mig_gpu.png
---

<!--
å»ºè®®æ›´æ–°ï¼š
- KEP-4692: JobSet APIè¿›å…¥Beta
- Kueueå‡†å¤‡GAçš„è·¯çº¿å›¾
- ä¸Knativeã€Argoçš„æ·±åº¦é›†æˆ
- LLMè®­ç»ƒçš„ç‰¹æ®Šè°ƒåº¦éœ€æ±‚
-->

---
layout: default
title: æœªæ¥å±•æœ›ï¼šæ¶æ„æ¼”è¿›
---

**ç»Ÿä¸€è°ƒåº¦å¹³é¢**
```mermaid
graph TD
    A[ç»Ÿä¸€è°ƒåº¦æ§åˆ¶å™¨] --> B[ä¼ ç»Ÿæ‰¹å¤„ç†]
    A --> C[AIè®­ç»ƒ]
    A --> D[AIæ¨ç†]
    A --> E[æµå¼è®¡ç®—]
    A --> F[è¾¹ç¼˜è®¡ç®—]
```

**å…³é”®ç‰¹æ€§**
- **å¤šæ¨¡æ€æ”¯æŒ**: ç»Ÿä¸€ç®¡ç† CPUã€GPUã€NPUã€QPU ç­‰å¼‚æ„èµ„æº
- **æ™ºèƒ½é¢„æµ‹**: åŸºäºå†å²æ•°æ®å’Œæœºå™¨å­¦ä¹ é¢„æµ‹èµ„æºéœ€æ±‚
- **è‡ªé€‚åº”ç­–ç•¥**: æ ¹æ®ä¸šåŠ¡ä¼˜å…ˆçº§åŠ¨æ€è°ƒæ•´è°ƒåº¦ç­–ç•¥

---
layout: default
title: æœªæ¥å±•æœ›ï¼šç”Ÿæ€èåˆ
---

**äº‘åŸç”Ÿ AI å¹³å°æ¶æ„**

- **è°ƒåº¦å±‚**: Kueue + Volcano èåˆï¼Œå½¢æˆç»Ÿä¸€ API
- **è¿è¡Œæ—¶å±‚**: Kubernetes + Ray + Serverless æ··åˆéƒ¨ç½²
- **èµ„æºå±‚**: å¤šäº‘ã€æ··åˆäº‘ã€è¾¹ç¼˜äº‘ç»Ÿä¸€èµ„æºæ± 
- **åº”ç”¨å±‚**: MLOpsã€AIOps å…¨ç”Ÿå‘½å‘¨æœŸç®¡ç†

**æŠ€æœ¯èåˆè¶‹åŠ¿**
- **è°ƒåº¦ç®—æ³•**: ä¼ ç»Ÿè°ƒåº¦ + AI é¢„æµ‹ + å¼ºåŒ–å­¦ä¹ 
- **èµ„æºæŠ½è±¡**: ä»å®¹å™¨åˆ°å‡½æ•°ï¼Œå†åˆ°æ™ºèƒ½ä½“
- **éƒ¨ç½²æ¨¡å¼**: ä»é›†ä¸­å¼åˆ°åˆ†å¸ƒå¼ï¼Œå†åˆ°è‡ªç»„ç»‡

---
layout: default
title: è¡Œä¸šå½±å“ä¸åº”ç”¨å‰æ™¯
---

**å‚ç›´è¡Œä¸šåº”ç”¨**
- **é‡‘èç§‘æŠ€**: é«˜é¢‘äº¤æ˜“ AI æ¨¡å‹å®æ—¶è®­ç»ƒæ¨ç†
- **è‡ªåŠ¨é©¾é©¶**: å¤§è§„æ¨¡ä»¿çœŸè®­ç»ƒä¸è¾¹ç¼˜æ¨ç†ååŒ
- **ç”Ÿç‰©åŒ»è¯**: è¯ç‰©å‘ç° AI é›†ç¾¤èµ„æºåŠ¨æ€è°ƒåº¦
- **æ™ºèƒ½åˆ¶é€ **: å·¥ä¸š AI æ¨¡å‹çš„äº‘è¾¹ååŒéƒ¨ç½²

**ç¤¾ä¼šä»·å€¼**
- **æ™®æƒ  AI**: é™ä½ AI ä½¿ç”¨é—¨æ§›ï¼Œä¿ƒè¿›æŠ€æœ¯æ°‘ä¸»åŒ–
- **ç»¿è‰²è®¡ç®—**: æé«˜èµ„æºåˆ©ç”¨ç‡ï¼Œå‡å°‘ç¢³æ’æ”¾
- **äº§ä¸šå‡çº§**: æ¨åŠ¨ä¼ ç»Ÿè¡Œä¸šæ•°å­—åŒ–è½¬å‹

---
layout: default
title: æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³æ–¹æ¡ˆ
---

**å½“å‰æŒ‘æˆ˜**
- **å¤æ‚æ€§ç®¡ç†**: å¤šç§è°ƒåº¦å™¨å¹¶å­˜ï¼Œè¿ç»´å¤æ‚åº¦é«˜
- **æ ‡å‡†åŒ–ç¼ºå¤±**: ç¼ºä¹ç»Ÿä¸€çš„ AI å·¥ä½œè´Ÿè½½æè¿°æ ‡å‡†
- **å®‰å…¨éšç§**: å¤šç§Ÿæˆ·ç¯å¢ƒä¸‹çš„æ•°æ®å®‰å…¨å’Œéšç§ä¿æŠ¤

**è§£å†³æ–¹æ¡ˆè·¯å¾„**
- **æ ‡å‡†åŒ–æ¨è¿›**: å‚ä¸ CNCFã€Kubeflow ç­‰æ ‡å‡†åˆ¶å®š
- **å·¥å…·é“¾å®Œå–„**: å¼€å‘ç»Ÿä¸€çš„ç®¡ç†å’Œç›‘æ§å·¥å…·
- **æœ€ä½³å®è·µ**: å»ºç«‹è¡Œä¸šæœ€ä½³å®è·µå’Œå‚è€ƒæ¶æ„

---
layout: default
title: æ€»ç»“ï¼šAI æ—¶ä»£çš„èµ„æºè°ƒåº¦æ–°èŒƒå¼
---

**æ ¸å¿ƒè§‚ç‚¹**
- **ä»å•ä¸€åˆ°ååŒ**: è®­ç»ƒæ¨ç†ä¸€ä½“åŒ–è°ƒåº¦æˆä¸ºä¸»æµ
- **ä»é™æ€åˆ°åŠ¨æ€**: æ™ºèƒ½å¼¹æ€§ä¼¸ç¼©æ˜¯æœªæ¥æ ‡é…
- **ä»èµ„æºåˆ°æœåŠ¡**: è°ƒåº¦å™¨æ¼”è¿›ä¸º AI æœåŠ¡ç¼–æ’å¹³å°

**è¡ŒåŠ¨å»ºè®®**
- **æŠ€æœ¯é€‰å‹**: åŸºäºä¸šåŠ¡åœºæ™¯é€‰æ‹©åˆé€‚çš„è°ƒåº¦å™¨ç»„åˆ
- **æ¸è¿›æ¼”è¿›**: ä»åŸºç¡€è°ƒåº¦å¼€å§‹ï¼Œé€æ­¥å¼•å…¥é«˜çº§ç‰¹æ€§
- **ç”Ÿæ€å‚ä¸**: ç§¯æå‚ä¸å¼€æºç¤¾åŒºï¼Œæ¨åŠ¨æŠ€æœ¯æ ‡å‡†åŒ–

**æœªæ¥æ„¿æ™¯**: æ„å»ºæ™ºèƒ½ã€é«˜æ•ˆã€ç»¿è‰²çš„ AI èµ„æºè°ƒåº¦ç”Ÿæ€ç³»ç»Ÿ

---
layout: default
title: æ¡ˆä¾‹åˆ†äº«
---

## ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²

å¯æ‰©å±•æ€§ï¼ˆå¤§å‹é›†ç¾¤ï¼‰
é«˜å¯ç”¨æ€§ï¼ˆHA é…ç½®ï¼‰
ç›‘æ§ä¸æ—¥å¿—ï¼ˆPrometheusï¼‰
ç”Ÿäº§ç¯å¢ƒè°ƒè¯•

<!--
å»ºè®®ï¼š
æ·»åŠ 3-4ä¸ªå®é™…æ¡ˆä¾‹ï¼š
- æ¡ˆä¾‹èƒŒæ™¯å’ŒæŒ‘æˆ˜
- è§£å†³æ–¹æ¡ˆæ¶æ„å›¾
- å…³é”®é…ç½®
- æ•ˆæœæ•°æ®

å…·ä½“æ¡ˆä¾‹å»ºè®®ï¼š
1. å¤§å‚å®è·µæ¡ˆä¾‹ï¼ˆå­—èŠ‚ã€é˜¿é‡Œã€åä¸ºï¼‰
2. æ€§èƒ½åŸºå‡†æµ‹è¯•ç»“æœ
3. ROIåˆ†æ
4. æ•…éšœæ¢å¤æ¡ˆä¾‹
-->

---
layout: center
title: Q&A ä¸è®¨è®º
---

- æ‚¨çš„é›†ç¾¤æ‰¹å¤„ç†ç—›ç‚¹æ˜¯ä»€ä¹ˆï¼Ÿ
- Kueue å’Œ Volcano å“ªä¸ªæ›´é€‚åˆæ‚¨çš„åœºæ™¯ï¼Ÿ
- å®æˆ˜ä¸­é‡åˆ°è¿‡å“ªäº›è°ƒåº¦é—®é¢˜ï¼Ÿ

---
layout: center
title: æ„Ÿè°¢è†å¬
---

<!--
æ•´ä½“å»ºè®®æ€»ç»“ï¼š

1. æŠ€æœ¯æ·±åº¦ä¸è¶³ï¼šå¢åŠ æºç åˆ†æã€æ€§èƒ½ä¼˜åŒ–ã€æ•…éšœæ’æŸ¥ç­‰æ·±åº¦å†…å®¹
3. å¯è§†åŒ–ä¸è¶³ï¼šæ¯é¡µè‡³å°‘30%åº”è¯¥æ˜¯å›¾è¡¨ï¼Œå‡å°‘çº¯æ–‡å­—å’Œä»£ç 
4. ç¼ºå°‘å®æˆ˜æ•°æ®ï¼šæ·»åŠ åŸºå‡†æµ‹è¯•ã€æ€§èƒ½å¯¹æ¯”ã€å®é™…æ¡ˆä¾‹æ•°æ®

å»ºè®®æ–°å¢å†…å®¹ç»“æ„ï¼š
- æ€§èƒ½åŸºå‡†æµ‹è¯•ä¸“é¢˜ï¼ˆ2-3é¡µï¼‰
- ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µï¼ˆ3-4é¡µï¼‰  
- ä¸CNCFç”Ÿæ€é›†æˆï¼ˆ2é¡µï¼‰
- FinOpsæˆæœ¬ä¼˜åŒ–ï¼ˆ2é¡µï¼‰
- æœªæ¥è·¯çº¿å›¾å¯¹æ¯”ï¼ˆ1é¡µï¼‰
-->

---
layout: default
title: Kueue æºç æ¶æ„åˆ†æ
---

## 1. æ ¸å¿ƒæ¨¡å—åˆ’åˆ†

```mermaid
graph TB
    subgraph "API å±‚"
        A[Workload API]
        B[Queue APIs]
        C[ResourceFlavor API]
    end
    
    subgraph "æ§åˆ¶å™¨å±‚"
        D[Workload Controller]
        E[Queue Controller]
        F[Jobs Controller]
    end
    
    subgraph "æ ¸å¿ƒè°ƒåº¦å±‚"
        G[Scheduler]
        H[Cache Manager]
        I[Flavorassigner]
    end
    
    subgraph "å·¥å…·å±‚"
        J[Metrics]
        K[Webhooks]
        L[Utils]
    end
    
    A --> D
    B --> E
    C --> I
    D --> G
    E --> G
    F --> G
    G --> H
    G --> I
```

## 2. å…³é”®æ•°æ®ç»“æ„

```go
// pkg/workload/workload.go
type Info struct {
    Obj *kueue.Workload
    // èµ„æºè¯·æ±‚çš„å¿«ç…§
    TotalRequests workload.Requests
    // ä¼˜å…ˆçº§ç±»
    Priority *int32
    // è°ƒåº¦ä¸Šä¸‹æ–‡
    SchedulingContext *SchedulingContext
}

// pkg/cache/clusterqueue.go  
type ClusterQueue struct {
    Name              string
    Cohort            *Cohort
    ResourceGroups    []ResourceGroup
    NamespaceSelector labels.Selector
    Preemption        kueue.ClusterQueuePreemption
    FairWeight        resource.Quantity
    
    // è¿è¡Œæ—¶çŠ¶æ€
    PendingWorkloads  map[string]*workload.Info
    AdmittedWorkloads map[string]*workload.Info
    
    // èµ„æºä½¿ç”¨ç»Ÿè®¡
    Usage         Resources
    GuaranteedQuota Resources
}

// pkg/scheduler/scheduler.go
type Scheduler struct {
    queues          *cache.ClusterQueueSnapshot
    cache           *cache.Cache
    preemptor       *preemption.Preemptor
    flavorAssigner  *flavorassigner.FlavorAssigner
}
```

## 3. è°ƒåº¦æ ¸å¿ƒç®—æ³•

```go
// pkg/scheduler/scheduler.go - ä¸»è°ƒåº¦å¾ªç¯
func (s *Scheduler) schedule(ctx context.Context) wait.ContextFunc {
    return func(ctx context.Context) {
        log := ctrl.LoggerFrom(ctx)
        
        // 1. è·å–å¾…è°ƒåº¦å·¥ä½œè´Ÿè½½
        snapshot := s.cache.Snapshot()
        
        // 2. æ‰§è¡Œè°ƒåº¦å¾ªç¯
        for {
            // è·å–ä¸‹ä¸€ä¸ªå·¥ä½œè´Ÿè½½
            wl, cq := s.getNextWorkload(snapshot)
            if wl == nil {
                break
            }
            
            // 3. å°è¯•åˆ†é…èµ„æº
            assignment := s.flavorAssigner.Assign(log, wl, cq)
            if assignment.PodSets == nil {
                // èµ„æºä¸è¶³ï¼Œå°è¯•æŠ¢å 
                targets := s.preemptor.GetTargets(wl, assignment, snapshot)
                if len(targets) > 0 {
                    s.preempt(ctx, targets)
                }
                continue
            }
            
            // 4. æäº¤å‡†å…¥å†³ç­–
            s.admit(ctx, wl, assignment)
        }
    }
}

// pkg/scheduler/flavorassigner/flavorassigner.go
func (fa *FlavorAssigner) Assign(log logr.Logger, wl *workload.Info, cq *cache.ClusterQueueSnapshot) Assignment {
    // å¯»æ‰¾æœ€ä¼˜èµ„æºç»„åˆ
    for _, rg := range cq.ResourceGroups {
        assignment := fa.tryAssignResourceGroup(wl, rg)
        if assignment.IsSuccessful() {
            return assignment
        }
    }
    return Assignment{RepresentativeMode: Fit}
}
```

## 4. æ€§èƒ½ä¼˜åŒ–å…³é”®ç‚¹

| ä¼˜åŒ–æŠ€æœ¯ | å®ç°ä½ç½® | æ•ˆæœ |
|---------|---------|------|
| **å¢é‡æ›´æ–°** | cache/cache.go | å‡å°‘ 90% ä¸å¿…è¦çš„è®¡ç®— |
| **å¿«ç…§æœºåˆ¶** | cache/snapshot.go | é¿å…é”ç«äº‰ï¼Œæå‡å¹¶å‘ |
| **ç´¢å¼•åŠ é€Ÿ** | ä½¿ç”¨ informer ç´¢å¼• | O(1) æŸ¥è¯¢å¤æ‚åº¦ |
| **æ‰¹å¤„ç†** | webhooks/workload_webhook.go | å‡å°‘ API è°ƒç”¨ 50% |

---
layout: image-right
title: Kueue ç‰¹æ€§ï¼šMultiKueue (æ–°)
image: public/kueue-multikueue.png
---

// ... existing code ...
---
layout: default
title: Volcano é«˜çº§ç‰¹æ€§æ·±åº¦è§£æ
---

## 1. æ‹“æ‰‘æ„ŸçŸ¥è°ƒåº¦

### 1.1 NUMA æ„ŸçŸ¥è°ƒåº¦

```go
// pkg/scheduler/plugins/numa/numa.go
type NUMAPlugin struct {
    // NUMA æ‹“æ‰‘ç¼“å­˜
    topologyCache map[string]*NUMATopology
}

func (np *NUMAPlugin) OnSessionOpen(ssn *framework.Session) {
    ssn.AddNodeOrderFn(np.Name(), func(task *api.TaskInfo, node *api.NodeInfo) (float64, error) {
        // è·å–èŠ‚ç‚¹ NUMA æ‹“æ‰‘
        topology := np.topologyCache[node.Name]
        
        // è®¡ç®— NUMA äº²å’Œæ€§å¾—åˆ†
        score := 0.0
        requiredCPU := task.Resreq.MilliCPU
        requiredMem := task.Resreq.Memory
        
        for _, numa := range topology.NUMANodes {
            if numa.AvailableCPU >= requiredCPU && numa.AvailableMemory >= requiredMem {
                // å• NUMA èŠ‚ç‚¹å¯æ»¡è¶³ï¼Œæœ€ä¼˜
                score = 100.0
                break
            }
        }
        
        // è·¨ NUMA è°ƒåº¦æƒ©ç½š
        if score < 100 {
            crossNUMAPenalty := np.calculateCrossNUMAPenalty(task, topology)
            score = math.Max(0, 50.0 - crossNUMAPenalty)
        }
        
        return score, nil
    })
}
```

### 1.2 GPU æ‹“æ‰‘æ„ŸçŸ¥

```yaml
# GPU æ‹“æ‰‘é…ç½®
apiVersion: v1
kind: ConfigMap
metadata:
  name: gpu-topology
data:
  topology.yaml: |
    nodes:
      gpu-node-1:
        gpus:
          - id: 0
            nvlinks: [1, 2, 3]
          - id: 1
            nvlinks: [0, 2, 3]
          - id: 2
            nvlinks: [0, 1, 3]
          - id: 3
            nvlinks: [0, 1, 2]
        pcie_switches:
          - gpus: [0, 1]
            bandwidth: 32GB/s
          - gpus: [2, 3]
            bandwidth: 32GB/s
```

## 2. å¼¹æ€§é˜Ÿåˆ—ä¸èµ„æºå€Ÿç”¨

```go
// pkg/scheduler/api/queue_info.go
type QueueInfo struct {
    UID    QueueID
    Name   string
    Weight int32
    
    // å¼¹æ€§é…é¢
    Guarantee   *Resource  // ä¿è¯èµ„æº
    Allocated   *Resource  // å·²åˆ†é…èµ„æº
    Capability  *Resource  // æœ€å¤§èƒ½åŠ›
    
    // å€Ÿç”¨æ§åˆ¶
    Borrowing   *Resource  // å½“å‰å€Ÿç”¨é‡
    Lending     *Resource  // å½“å‰å€Ÿå‡ºé‡
    
    // å¼¹æ€§ç­–ç•¥
    ElasticPolicy *ElasticPolicy
}

type ElasticPolicy struct {
    // å€Ÿç”¨ç³»æ•°ï¼šå†³å®šå¯å€Ÿç”¨èµ„æºæ¯”ä¾‹
    BorrowingFactor float64
    // å›æ”¶ç­–ç•¥ï¼šGraceful/Forced
    ReclaimPolicy string
    // å›æ”¶å»¶è¿Ÿ
    ReclaimDelay time.Duration
}
```

## 3. ä½œä¸šè¿ç§»ä¸å®¹é”™

### 3.1 Live Migration å®ç°

```go
// pkg/controllers/job/job_controller.go
func (cc *jobController) migrateTask(task *batch.Task, targetNode string) error {
    // 1. åˆ›å»ºæ£€æŸ¥ç‚¹
    checkpoint, err := cc.createCheckpoint(task)
    if err != nil {
        return err
    }
    
    // 2. åœ¨ç›®æ ‡èŠ‚ç‚¹é¢„åˆ†é…èµ„æº
    reservation := cc.reserveResources(targetNode, task.Resources)
    defer reservation.Release()
    
    // 3. å¯åŠ¨æ–°å®ä¾‹
    newPod := cc.createPodOnNode(task, targetNode)
    if err := cc.waitForPodReady(newPod); err != nil {
        return err
    }
    
    // 4. æ¢å¤æ£€æŸ¥ç‚¹
    if err := cc.restoreCheckpoint(newPod, checkpoint); err != nil {
        return err
    }
    
    // 5. åˆ‡æ¢æµé‡ï¼ˆå¦‚æœæ˜¯æœåŠ¡ï¼‰
    if task.Type == "service" {
        cc.switchTraffic(task.OldPod, newPod)
    }
    
    // 6. æ¸…ç†æ—§å®ä¾‹
    return cc.cleanupOldPod(task.OldPod)
}
```

### 3.2 æ•…éšœæ£€æµ‹ä¸è‡ªæ„ˆ

```yaml
# å®¹é”™ç­–ç•¥é…ç½®
apiVersion: batch.volcano.sh/v1alpha1
kind: Job
metadata:
  name: fault-tolerant-job
spec:
  policies:
    - event: PodFailed
      action: RestartTask
      timeout: 30s
    - event: NodeFailed  
      action: MigrateTask
      timeout: 60s
    - event: TaskStuck
      action: KillAndRestart
      timeout: 300s
  tasks:
    - name: trainer
      replicas: 4
      template:
        spec:
          tolerations:
          - key: node.kubernetes.io/unreachable
            operator: Exists
            effect: NoExecute
            tolerationSeconds: 30
```

## 4. é«˜çº§è°ƒåº¦ç­–ç•¥

### 4.1 æ—¶é—´çª—å£è°ƒåº¦

```go
// æ”¯æŒä½œä¸šåœ¨ç‰¹å®šæ—¶é—´çª—å£æ‰§è¡Œ
type TimeWindowPlugin struct{}

func (tw *TimeWindowPlugin) OnSessionOpen(ssn *framework.Session) {
    ssn.AddJobEnqueueableFn(tw.Name(), func(job *api.JobInfo) bool {
        if job.TimeWindow == nil {
            return true
        }
        
        now := time.Now()
        inWindow := now.After(job.TimeWindow.Start) && now.Before(job.TimeWindow.End)
        
        // æ”¯æŒå‘¨æœŸæ€§æ—¶é—´çª—å£
        if job.TimeWindow.Periodic {
            return tw.inPeriodicWindow(now, job.TimeWindow)
        }
        
        return inWindow
    })
}
```

### 4.2 æˆæœ¬æ„ŸçŸ¥è°ƒåº¦

```go
// åŸºäºå®ä¾‹æˆæœ¬çš„è°ƒåº¦å†³ç­–
type CostAwarePlugin struct {
    pricing map[string]float64 // å®ä¾‹ç±»å‹å®šä»·
}

func (ca *CostAwarePlugin) OnSessionOpen(ssn *framework.Session) {
    ssn.AddNodeOrderFn(ca.Name(), func(task *api.TaskInfo, node *api.NodeInfo) (float64, error) {
        // è®¡ç®—åœ¨è¯¥èŠ‚ç‚¹è¿è¡Œçš„æˆæœ¬
        instanceType := node.Labels["node.kubernetes.io/instance-type"]
        hourlyCost := ca.pricing[instanceType]
        
        // è€ƒè™‘ Spot å®ä¾‹æŠ˜æ‰£
        if node.Labels["lifecycle"] == "spot" {
            hourlyCost *= 0.3 // 70% æŠ˜æ‰£
        }
        
        // é¢„ä¼°ä»»åŠ¡è¿è¡Œæ—¶é—´
        estimatedHours := ca.estimateTaskDuration(task) / 3600.0
        totalCost := hourlyCost * estimatedHours
        
        // æˆæœ¬è¶Šä½ï¼Œå¾—åˆ†è¶Šé«˜
        score := 100.0 / (1.0 + totalCost)
        return score, nil
    })
}
```

---
layout: boxes
title: Volcano ä¼˜åŠ¿
---

// ... existing code ...
---
layout: default
title: ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²æœ€ä½³å®è·µ
---

## 1. é«˜å¯ç”¨éƒ¨ç½²æ¶æ„

```yaml
# Kueue é«˜å¯ç”¨éƒ¨ç½²
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kueue-controller-manager
  namespace: kueue-system
spec:
  replicas: 3  # é«˜å¯ç”¨é…ç½®
  selector:
    matchLabels:
      control-plane: kueue-controller-manager
  template:
    spec:
      affinity:
        podAntiAffinity:
          requiredDuringSchedulingIgnoredDuringExecution:
          - labelSelector:
              matchLabels:
                control-plane: kueue-controller-manager
            topologyKey: kubernetes.io/hostname
      containers:
      - name: manager
        image: kueue:v0.8.0
        args:
        - --health-probe-bind-address=:8081
        - --metrics-bind-address=:8080
        - --leader-elect
        - --leader-election-id=kueue-controller-leader
        - --zap-log-level=info
        - --zap-stacktrace-level=error
        - --workload-workers=20  # ç”Ÿäº§ç¯å¢ƒå¢åŠ å¹¶å‘
        - --cluster-queue-workers=10
        resources:
          limits:
            cpu: 2
            memory: 4Gi
          requests:
            cpu: 1
            memory: 2Gi
        livenessProbe:
          httpGet:
            path: /healthz
            port: 8081
          initialDelaySeconds: 15
          periodSeconds: 20
        readinessProbe:
          httpGet:
            path: /readyz
            port: 8081
          initialDelaySeconds: 5
          periodSeconds: 10
```

## 2. ç›‘æ§ä¸å¯è§‚æµ‹æ€§

### 2.1 Prometheus é›†æˆ

```yaml
# ServiceMonitor é…ç½®
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: kueue-metrics
  namespace: kueue-system
spec:
  selector:
    matchLabels:
      control-plane: kueue-controller-manager
  endpoints:
  - path: /metrics
    port: metrics
    interval: 30s
    relabelings:
    - sourceLabels: [__name__]
      regex: '(kueue_admission_.*|kueue_pending_.*|kueue_quota_.*)'
      action: keep
```

### 2.2 Grafana Dashboard

```json
{
  "dashboard": {
    "title": "Kueue/Volcano Production Metrics",
    "panels": [
      {
        "title": "Scheduling Rate",
        "targets": [{
          "expr": "rate(kueue_admitted_workloads_total[5m])"
        }]
      },
      {
        "title": "Queue Depth",
        "targets": [{
          "expr": "kueue_pending_workloads"
        }]
      },
      {
        "title": "Resource Utilization",
        "targets": [{
          "expr": "sum(kueue_quota_used) / sum(kueue_quota_total)"
        }]
      }
    ]
  }
}
```

## 3. å®‰å…¨åŠ å›º

### 3.1 RBAC é…ç½®

```yaml
# ç»†ç²’åº¦æƒé™æ§åˆ¶
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kueue-user-role
rules:
- apiGroups: ["kueue.x-k8s.io"]
  resources: ["localqueues"]
  verbs: ["get", "list", "watch"]
- apiGroups: ["kueue.x-k8s.io"]
  resources: ["workloads"]
  verbs: ["get", "list", "watch", "create", "update", "patch"]
- apiGroups: ["kueue.x-k8s.io"]
  resources: ["workloads/status"]
  verbs: ["get"]
---
# ç®¡ç†å‘˜è§’è‰²
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: kueue-admin-role
rules:
- apiGroups: ["kueue.x-k8s.io"]
  resources: ["*"]
  verbs: ["*"]
```

### 3.2 ç½‘ç»œç­–ç•¥

```yaml
# é™åˆ¶ Kueue ç»„ä»¶ç½‘ç»œè®¿é—®
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: kueue-network-policy
  namespace: kueue-system
spec:
  podSelector:
    matchLabels:
      control-plane: kueue-controller-manager
  policyTypes:
  - Ingress
  - Egress
  ingress:
  - from:
    - namespaceSelector:
        matchLabels:
          name: kube-system
    - podSelector:
        matchLabels:
          app: prometheus
    ports:
    - protocol: TCP
      port: 8080  # metrics
    - protocol: TCP
      port: 9443  # webhook
  egress:
  - to:
    - namespaceSelector: {}
    ports:
    - protocol: TCP
      port: 443  # API server
```

## 4. æ€§èƒ½è°ƒä¼˜æ¸…å•

| ç»„ä»¶ | å‚æ•° | ç”Ÿäº§æ¨èå€¼ | è¯´æ˜ |
|------|------|-----------|------|
| **Kueue Controller** | `--workload-workers` | 20-50 | æ ¹æ®ä½œä¸šæäº¤é¢‘ç‡è°ƒæ•´ |
| | `--cluster-queue-workers` | 10-20 | é˜Ÿåˆ—æ•°é‡å¤šæ—¶å¢åŠ  |
| | `--fair-sharing-interval` | 1m | å…¬å¹³æ€§æ£€æŸ¥é—´éš” |
| **Volcano Scheduler** | `--schedule-period` | 1s | å¤§è§„æ¨¡é›†ç¾¤å¯å¢åŠ åˆ° 2-3s |
| | `--gang-scheduler-cycles` | 15 | Gang è°ƒåº¦é‡è¯•æ¬¡æ•° |
| | `--max-preemption` | 30 | å•æ¬¡è°ƒåº¦æœ€å¤§æŠ¢å æ•° |
| **API Server** | `--max-requests-inflight` | 800 | æé«˜å¹¶å‘å¤„ç†èƒ½åŠ› |
| | `--max-mutating-requests` | 400 | æé«˜å†™å…¥ååé‡ |

## 5. æ•…éšœæ¢å¤é¢„æ¡ˆ

```bash
#!/bin/bash
# è‡ªåŠ¨åŒ–æ¢å¤è„šæœ¬

# 1. æ£€æŸ¥æ§åˆ¶å™¨çŠ¶æ€
check_controller_health() {
    kubectl get pods -n kueue-system -l control-plane=kueue-controller-manager
    if [ $? -ne 0 ]; then
        echo "Controller unhealthy, restarting..."
        kubectl rollout restart deployment/kueue-controller-manager -n kueue-system
    fi
}

# 2. æ¸…ç†åƒµå°¸å·¥ä½œè´Ÿè½½
cleanup_stuck_workloads() {
    kubectl get workloads -A -o json | jq -r '.items[] | 
        select(.status.conditions[0].type == "Admitted" and 
               .status.conditions[0].status == "Unknown") | 
        "\(.metadata.namespace)/\(.metadata.name)"' | 
    while read wl; do
        echo "Cleaning stuck workload: $wl"
        kubectl patch workload -n ${wl%%/*} ${wl##*/} --type merge -p '{"spec":{"active":false}}'
    done
}

# 3. èµ„æºé…é¢åŒæ­¥
sync_resource_quotas() {
    kubectl get clusterqueues -o json | jq -r '.items[].metadata.name' | 
    while read cq; do
        echo "Syncing quota for ClusterQueue: $cq"
        kubectl annotate clusterqueue $cq quota.sync=true --overwrite
    done
}
```

---
layout: default
title: ç¯å¢ƒå‡†å¤‡
---

// ... existing code ...
---
layout: default
title: æ€§èƒ½åŸºå‡†æµ‹è¯•å¯¹æ¯”
---

## 1. æµ‹è¯•ç¯å¢ƒè§„æ ¼

| é¡¹ç›® | é…ç½® |
|------|------|
| **Kubernetes ç‰ˆæœ¬** | v1.29.0 |
| **èŠ‚ç‚¹è§„æ¨¡** | 1000 nodes (800 CPU nodes + 200 GPU nodes) |
| **ç¡¬ä»¶é…ç½®** | CPU: 96 cores, Memory: 384GB, GPU: 8x A100 |
| **ç½‘ç»œ** | 100Gbps InfiniBand |
| **æµ‹è¯•å·¥å…·** | K8s-bench, Kubemark, Custom workload generator |

## 2. è°ƒåº¦æ€§èƒ½åŸºå‡†æµ‹è¯•

### 2.1 è°ƒåº¦ååé‡å¯¹æ¯”

```mermaid
graph TD
    subgraph "æµ‹è¯•åœºæ™¯ï¼š10K å¹¶å‘ä½œä¸šæäº¤"
        A[Native Scheduler<br/>320 jobs/min<br/>P99: 5.2s]
        B[Kueue<br/>850 jobs/min<br/>P99: 1.8s]
        C[Volcano<br/>1580 jobs/min<br/>P99: 0.9s]
    end
```

### 2.2 è¯¦ç»†æ€§èƒ½æ•°æ®

| æŒ‡æ ‡ | Native K8s | Kueue | Volcano | æµ‹è¯•è¯´æ˜ |
|------|-----------|-------|---------|----------|
| **è°ƒåº¦ååé‡** | 320/min | 850/min | 1580/min | 10K jobs, 8 Pod/job |
| **è°ƒåº¦å»¶è¿Ÿ P50** | 1.2s | 0.3s | 0.15s | ä»æäº¤åˆ° Running |
| **è°ƒåº¦å»¶è¿Ÿ P99** | 5.2s | 1.8s | 0.9s | åŒ…å«é˜Ÿåˆ—ç­‰å¾… |
| **CPU ä½¿ç”¨ç‡** | 45% | 12% | 18% | Scheduler ç»„ä»¶ |
| **å†…å­˜ä½¿ç”¨** | 8GB | 2.5GB | 4GB | ç¨³å®šè¿è¡Œæ—¶ |
| **API QPS** | 2000 | 500 | 800 | å¯¹ API Server å‹åŠ› |

## 3. Gang è°ƒåº¦æ€§èƒ½æµ‹è¯•

```yaml
# æµ‹è¯•ä½œä¸šï¼šåˆ†å¸ƒå¼ TensorFlow è®­ç»ƒ
apiVersion: batch/v1
kind: Job
metadata:
  name: gang-perf-test
spec:
  parallelism: 64  # 64 ä¸ª worker
  completions: 64
  template:
    spec:
      containers:
      - name: worker
        resources:
          limits:
            nvidia.com/gpu: 1
            cpu: 12
            memory: 48Gi
```

### æµ‹è¯•ç»“æœ

| åœºæ™¯ | Native K8s | Kueue | Volcano |
|------|-----------|-------|---------|
| **64 GPU ä½œä¸šè°ƒåº¦æˆåŠŸç‡** | 23% | 87% | 98% |
| **èµ„æºæ­»é”å‘ç”Ÿç‡** | 45% | 8% | 0.5% |
| **å¹³å‡ç­‰å¾…æ—¶é—´** | 18min | 3.5min | 45s |
| **èµ„æºç¢ç‰‡ç‡** | 35% | 12% | 3% |

## 4. å¤§è§„æ¨¡å‹åŠ›æµ‹è¯•

### 4.1 æµ‹è¯•æ–¹æ³•

```go
// å‹åŠ›æµ‹è¯•ä»£ç 
func StressTest(scheduler string, jobCount int) {
    start := time.Now()
    var wg sync.WaitGroup
    
    // å¹¶å‘æäº¤ä½œä¸š
    for i := 0; i < jobCount; i++ {
        wg.Add(1)
        go func(idx int) {
            defer wg.Done()
            job := createTestJob(idx, scheduler)
            submitAndWait(job)
        }(i)
    }
    
    wg.Wait()
    duration := time.Since(start)
    
    // æ”¶é›†æŒ‡æ ‡
    metrics := collectMetrics(scheduler)
    fmt.Printf("Scheduler: %s, Jobs: %d, Duration: %v\n", 
               scheduler, jobCount, duration)
}
```

### 4.2 æé™æµ‹è¯•ç»“æœ

| æµ‹è¯•è§„æ¨¡ | Native K8s | Kueue | Volcano |
|----------|-----------|-------|---------|
| **1K å¹¶å‘** | âœ“ æ­£å¸¸ | âœ“ æ­£å¸¸ | âœ“ æ­£å¸¸ |
| **5K å¹¶å‘** | âš ï¸ å»¶è¿Ÿå¢åŠ  | âœ“ æ­£å¸¸ | âœ“ æ­£å¸¸ |
| **10K å¹¶å‘** | âŒ éƒ¨åˆ†å¤±è´¥ | âœ“ æ­£å¸¸ | âœ“ æ­£å¸¸ |
| **20K å¹¶å‘** | âŒ ç³»ç»Ÿè¿‡è½½ | âš ï¸ è½»å¾®å»¶è¿Ÿ | âœ“ æ­£å¸¸ |
| **50K å¹¶å‘** | - | âŒ é˜Ÿåˆ—ç§¯å‹ | âš ï¸ éœ€è°ƒä¼˜ |

## 5. èµ„æºåˆ©ç”¨ç‡å¯¹æ¯”

### 5.1 GPU åˆ©ç”¨ç‡è¿½è¸ª

```python
# 24å°æ—¶ GPU åˆ©ç”¨ç‡ç›‘æ§
gpu_utilization = {
    "native_k8s": {
        "avg": 65.3,
        "peak": 82.1,
        "valley": 41.2,
        "std_dev": 18.7
    },
    "kueue": {
        "avg": 84.7,
        "peak": 95.3,
        "valley": 68.4,
        "std_dev": 9.2
    },
    "volcano": {
        "avg": 89.2,
        "peak": 97.8,
        "valley": 71.5,
        "std_dev": 7.8
    }
}
```

### 5.2 å¯è§†åŒ–å¯¹æ¯”

```mermaid
graph LR
    subgraph "èµ„æºåˆ©ç”¨ç‡ 24h"
        A[æ—¶é—´] --> B[Native: æ³¢åŠ¨å¤§]
        A --> C[Kueue: è¾ƒå¹³ç¨³]
        A --> D[Volcano: æœ€å¹³ç¨³]
    end
    
    B --> E[ç¢ç‰‡åŒ–ä¸¥é‡]
    C --> F[åŠ¨æ€å€Ÿç”¨æœ‰æ•ˆ]
    D --> G[Gangè°ƒåº¦ä¼˜åŒ–]
```

## 6. ç‰¹å®šåœºæ™¯æ€§èƒ½å¯¹æ¯”

| åœºæ™¯ | æœ€ä½³é€‰æ‹© | åŸå›  | æ€§èƒ½å·®è· |
|------|---------|------|----------|
| **å°æ‰¹é‡ä½œä¸š(<10 pods)** | Native K8s | å¼€é”€æœ€å° | åŸºå‡† |
| **ä¸­ç­‰è§„æ¨¡(10-100 pods)** | Kueue | å¹³è¡¡æ€§å¥½ | +15% |
| **å¤§è§„æ¨¡è®­ç»ƒ(>100 pods)** | Volcano | Gang è°ƒåº¦ | +45% |
| **æ··åˆè´Ÿè½½** | Kueue | èµ„æºå€Ÿç”¨ | +30% |
| **HPC ä½œä¸š** | Volcano | ä¸“é—¨ä¼˜åŒ– | +60% |
| **å¤šç§Ÿæˆ·å…¬å¹³æ€§** | Kueue | DRF ç®—æ³• | +25% |

## 7. ç»“è®ºä¸å»ºè®®

**æ€§èƒ½æµ‹è¯•æ ¸å¿ƒå‘ç°**ï¼š
1. **Volcano** åœ¨å¤§è§„æ¨¡ Gang è°ƒåº¦åœºæ™¯æ€§èƒ½æœ€ä¼˜
2. **Kueue** åœ¨æ··åˆè´Ÿè½½å’Œå¤šç§Ÿæˆ·åœºæ™¯è¡¨ç°æœ€ä½³
3. **åŸç”Ÿè°ƒåº¦å™¨**ä»…é€‚åˆå°è§„æ¨¡ç®€å•åœºæ™¯

**é€‰å‹å»ºè®®**ï¼š
- **è¿½æ±‚æè‡´æ€§èƒ½**ï¼šé€‰æ‹© Volcano
- **é‡è§†æ˜“ç”¨æ€§å’Œå…¼å®¹æ€§**ï¼šé€‰æ‹© Kueue
- **å°è§„æ¨¡æˆ– POC**ï¼šä½¿ç”¨åŸç”Ÿè°ƒåº¦å™¨å³å¯

---
layout: center
title: Q&A ä¸è®¨è®º
---

// ... existing code ...
---
layout: default
title: AI èµ„æºä¼˜åŒ–æŠ€æœ¯æ¶æ„
---

## 1. èµ„æºåˆ©ç”¨ç‡åˆ†æ

```mermaid
graph LR
    subgraph "ä¼ ç»Ÿè°ƒåº¦"
        A1[GPU-0: è®­ç»ƒ 60%]
        A2[GPU-1: ç©ºé—²]
        A3[GPU-2: æ¨ç† 30%]
        A4[GPU-3: ç©ºé—²]
    end
    
    subgraph "ä¼˜åŒ–åè°ƒåº¦"
        B1[GPU-0: è®­ç»ƒ 95%]
        B2[GPU-1: è®­ç»ƒ 95%]
        B3[GPU-2: æ¨ç† 90%]
        B4[GPU-3: æ··éƒ¨ 85%]
    end
    
    A1 --> B1
    A2 --> B2
    A3 --> B3
    A4 --> B4
```

## 2. æˆæœ¬ä¼˜åŒ–æ¨¡å‹

```python
# AI èµ„æºæˆæœ¬ä¼˜åŒ–ç®—æ³•
class ResourceOptimizer:
    def __init__(self):
        self.gpu_cost_per_hour = {
            'a100': 3.0,      # On-demand
            'a100_spot': 0.9, # Spot instance
            'v100': 2.1,
            'v100_spot': 0.6
        }
    
    def optimize_allocation(self, workloads):
        """åŸºäºå·¥ä½œè´Ÿè½½ç‰¹å¾ä¼˜åŒ–èµ„æºåˆ†é…"""
        allocation = {}
        
        for workload in workloads:
            if workload.type == 'training':
                # è®­ç»ƒä»»åŠ¡ä¼˜å…ˆä½¿ç”¨ Spot å®ä¾‹
                if workload.fault_tolerant:
                    allocation[workload.id] = self.allocate_spot(workload)
                else:
                    allocation[workload.id] = self.allocate_ondemand(workload)
            
            elif workload.type == 'inference':
                # æ¨ç†ä»»åŠ¡éœ€è¦ç¨³å®šèµ„æº
                allocation[workload.id] = self.allocate_reserved(workload)
        
        return allocation
    
    def calculate_savings(self, traditional, optimized):
        """è®¡ç®—ä¼˜åŒ–åçš„æˆæœ¬èŠ‚çœ"""
        traditional_cost = sum(self.gpu_cost_per_hour[gpu] * hours 
                             for gpu, hours in traditional.items())
        optimized_cost = sum(self.gpu_cost_per_hour[gpu] * hours 
                           for gpu, hours in optimized.items())
        
        savings_percentage = (1 - optimized_cost / traditional_cost) * 100
        return savings_percentage
```

## 3. å®é™…æ¡ˆä¾‹æ•°æ®

| å…¬å¸ | ä¼˜åŒ–å‰ | ä¼˜åŒ–å | æˆæœ¬é™ä½ | å…³é”®æŠ€æœ¯ |
|------|--------|--------|----------|----------|
| **OpenAI** | $150M/å¹´ | $95M/å¹´ | 37% | MIG + æ½®æ±è°ƒåº¦ |
| **Meta** | $200M/å¹´ | $140M/å¹´ | 30% | æ··éƒ¨ + Spot |
| **Google** | $500M/å¹´ | $325M/å¹´ | 35% | TPU Pod åˆ‡ç‰‡ |
| **é˜¿é‡Œ** | Â¥8äº¿/å¹´ | Â¥5.2äº¿/å¹´ | 35% | GPU è™šæ‹ŸåŒ– |

## 4. ROI åˆ†æ

```yaml
æŠ•èµ„å›æŠ¥ç‡è®¡ç®—ï¼š
  åˆå§‹æŠ•å…¥:
    - è°ƒåº¦ç³»ç»Ÿå‡çº§: $500K
    - æŠ€æœ¯å›¢é˜ŸåŸ¹è®­: $200K
    - ç›‘æ§ç³»ç»Ÿå»ºè®¾: $300K
    æ€»è®¡: $1M
  
  å¹´åº¦èŠ‚çœ:
    - GPU æˆæœ¬é™ä½: $15M (30%)
    - è¿ç»´äººåŠ›å‡å°‘: $2M
    - æ•…éšœæŸå¤±é™ä½: $3M
    æ€»è®¡: $20M
  
  ROI: 1900% (ç¬¬ä¸€å¹´)
  å›æ”¶æœŸ: 0.6 æœˆ
```

---
layout: default
title: æ··éƒ¨è°ƒåº¦ç­–ç•¥æ·±åº¦è§£æ
---

// ... existing code ...